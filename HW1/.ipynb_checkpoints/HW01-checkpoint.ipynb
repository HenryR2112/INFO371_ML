{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7341ee3",
   "metadata": {},
   "source": [
    "# HW 1 - Gradient Descent with Linear Regression \n",
    "\n",
    "## Instructions\n",
    "In this homework assignment, we are now going to ask you to implement gradient descent from scratch! This assignment will follow a similar structure as the lab, but we are going to ask you to do a little more by hand to check that you understand what is going on at each step. We have given you all the equations (so you don't have to calculate the partial derivates yourself) -- your goal is to figure out how to piece all the equations together and plug in your data.\n",
    "\n",
    "Keep in mind that gradient descent is in the same family as hill climbing search -- so your approach to writting your code for this assignment will likely be similar as what you did in lab. Remeber that with hill climbing, you needed to find all your neighbors, then calculate the cost function for each neighbor, and then chose the neighbor with the lowest cost. HOWEVER, with gradient descent you just use calculus/gradients to iteratively improve your guesses for the slope and intercept.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817e64b",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent by Hand\n",
    "\n",
    "Let's start by doing gradient descent by hand. For this, you cannont use any python packages -- you should do this using your algebra skills and a calculator. I recommend doing this on pen and paper and then attaching a picture/scan of your work to this python notebook\n",
    "\n",
    "### Part 1: Only Estimate Slope\n",
    "For this example, let’s assume we are only trying to estimate one variable given one data point. Let’s just consider the slope $m$ of a line and one data point $x = 1$. \n",
    "\n",
    "For gradient descent, we’ll also need a couple of parameters set ahead of time: 1) starting slope, 2) the cost function (and its partial derivative w.r.t the parameter we are trying to estimate), and 3) the step size/learning rate. \n",
    "\n",
    "Let's start with $m = 3$, a cost function $cost(m) = m^3$, and a step size/learning rate $\\alpha = 0.5 $\n",
    "\n",
    "Using some calculus, the partial derivative of the cost function w.r.t. m is $\\frac{\\partial cost}{\\partial m} = 3m^2$. This derivative is what you will use to calculate the gradient. \n",
    "\n",
    "\n",
    "1. Using these starting variables and a starting data point of $x = 1$, what is the slope after the first iteration of gradient descent? (HINT: it should be less than 3)\n",
    "\n",
    "\n",
    "2. What is updated slope on the next iteration? and the third iteration? \n",
    "\n",
    "\n",
    "3. What is happening to the slope after each iteration? is it increasing, decreasing, staying the same? \n",
    "\n",
    "\n",
    "\n",
    "### Part 2: More Realistic Math\n",
    "Now lets increase the complexity slightly. Let's try to do gradient descent for both the slope and intercept at the same time AND lets have a dataset now with two data points: (0,1) and (3, 2)\n",
    "\n",
    "This time, lets also use a more standard cost function -- i.e. the mean square error function. Given our training data \"x\" (i.e. 0 and 3) and labels \"y\" (i.e. 1 and 2), the cost function (aka: mean squared error) is $cost(x) = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - (mx_i + b))^2$ where $n$ is the length of our dataset (in this case, $n$ = 2 because we only have two datapoints in this example). \n",
    "\n",
    "The partial derivatives of the cost function w.r.t m is $\\frac{\\partial cost}{\\partial m} = \\frac{-2}{n}\\sum_{i=1}^{n} x_i(y_i - (mx_i + b))$\n",
    "\n",
    "The partial derivatives of the cost function w.r.t b is $\\frac{\\partial cost}{\\partial b} = \\frac{-2}{n}\\sum_{i=1}^{n} y_i - (mx_i + b)$\n",
    "\n",
    "For this example, lets start with $m = 0$, $b = 0$, and $\\alpha = 0.01$\n",
    "\n",
    "\n",
    "\n",
    "3. Using these starting variables, what is the slope and intercept after the first iteration of gradient descent. \n",
    "\n",
    "\n",
    "\n",
    "4. What is slope and intercept on the next iteration? and the third iteration? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41210def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may use python here as a calculator "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec4666",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent with Code \n",
    "\n",
    "Now that we have done this by hand, lets work on implementing gradient descent from scratch! \n",
    "\n",
    "The general algorithm goes something like this: \n",
    "\n",
    "---\n",
    "\n",
    "*INPUT* : X values $x_1 \\dots x_n$\n",
    "\n",
    "\n",
    "*LOOP* until change in cost $\\leq$ stopping threshold OR  you've hit max # of iterations:\n",
    "    \n",
    "1. $m_{new} \\leftarrow m_{old} - \\alpha * \\frac{\\partial cost}{\\partial m}(m_{old}, x, y, b_{old})$\n",
    "2. $b_{new} \\leftarrow b_{old} - \\alpha * \\frac{\\partial cost}{\\partial b}(m_{old}, x, y, b_{old})$\n",
    "\n",
    "\n",
    "*RETURN* final m and b\n",
    "\n",
    "---\n",
    "\n",
    "5. Start by writting a function that calculates the gradient w.r.t slope. In the code block below, we have given you a starting point to work with. Your function should return the updated slope value using the gradient formula you used in Section 1 part 2. Use your answers in Section 1 part 2 to check your work! \n",
    "\n",
    "\n",
    "6. Then a funcion that calculates gradient w.r.t intercept. In the code block below, we have given you a starting point to work with. Your function should return the updated intercept value using the gradient formula you used in Section 1 part 2. Use your answers in Section 1 part 2 to check your work! \n",
    "\n",
    "\n",
    "\n",
    "7. Now write a function that does gradient descent. to make part 4 easier, you might want to have your function return how many iterations it took to get the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175270da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some needed import statements \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "956fbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y, y_pred):\n",
    "    return mean_squared_error(y, y_pred)\n",
    "\n",
    "def gradient_slope(x, y, y_pred):\n",
    "    #put code here for question 5\n",
    "    #you shouldn't need to change the parameters\n",
    "    return m_new\n",
    "\n",
    "def gradient_intercept(y, y_pred):\n",
    "    #put code here for question 6\n",
    "    #you shouldn't need to change the parameters\n",
    "    return b_new\n",
    "\n",
    "def grad_descent(x, y, max_iter, alpha, stop_thresh):\n",
    "    #set the starting values for slope and intercept to zero\n",
    "    m = 0 \n",
    "    b = 0\n",
    "    \n",
    "    #put code here for question 7\n",
    "    return m_new, b_new, num_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2e7b5",
   "metadata": {},
   "source": [
    "## 3. Lets Check our Work\n",
    "\n",
    "Let's use the same random dataset from our lab to check our work! \n",
    "\n",
    "8. Run gradient descent using the `random.csv` dataset and set the parameters as max_iter = 3000, alpha = 0.001, and stop_thresh = 1e-6. What does your gradient descent function return for the slope and intercept? Did it converge on the answer before it hit the max number of iterations?\n",
    "\n",
    "\n",
    "9. Create a scatter plot showing the datapoints from `random.csv` and draws a line showing the predicited line generated by your gradient descent function. Does your guess look correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here \n",
    "data = pd.read_csv(\"random.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8b27e",
   "metadata": {},
   "source": [
    "## 4. Experimenting with Different Parameters \n",
    "\n",
    "Finally lets experiment with changing the step size/learning rate (i.e the parameter alpha) and see how many iterations it takes for our approach to converge! \n",
    "\n",
    "10. Try a small alpha value of 0.001, medium alpha value 0.01, and a large alpha value 0.1. How many iterations for each alpha value does it take to converge on a solution? \n",
    "\n",
    "\n",
    "\n",
    "11. Now lets try an approach where the alpha value starts large and slowly gets smaller and smaller each iteration. Lets use the formula $\\alpha(t) = \\frac{1000}{1000 + t}$ where $t$ is the iteration number you are currently on (meaning in the first itneration t = 1, and so on). With this decaying learning rate, how many iterations does it take for your approach to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
