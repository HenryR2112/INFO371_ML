{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "36333990",
      "cell_type": "markdown",
      "source": "# Lab - Decision Trees \n\nThis lab asks you to play with regression and classification trees,\nand find the best combination of hyperparameters.  We use Wisconsin\nDiagnostic Breast Cancer (WDBC) data for categorization and Boston\nhousing data for regression.  Both tasks are fairly similar.\n\nThe aim of this lab is to give you some experience with trees and\nhyperparameter tuning.  Try to get as good accuracy/RMSE as possible!",
      "metadata": {}
    },
    {
      "id": "7865efc5",
      "cell_type": "markdown",
      "source": "## Classification \nIn this task you work with WDBC data.  As a reminder, your task is to\npredict __diagnosis__ (''M'' = cancer, ''B'' = no cancer).  \n\n\n1. Load wdbc data and ensure it looks good.\n\n\n2. Create your feature matrix $X$ and label vector $y$.  The former should contain all 30 features,  everything, except __diagnosis__ and __id__.  The latter should be __diagnosis__, converted to either logical or numeric variable (otherwise sklearn will fail).\n\n\n3.  Split your data into training and validation chunks (or do cross validation below, but that is slower).\n",
      "metadata": {}
    },
    {
      "id": "fabd5d07",
      "cell_type": "code",
      "source": "#code goes here\nimport pandas as pd\nimport numpy as np\nfrom sklearn import tree\nwdbc_df = pd.read_csv('wdbc.csv.bz2')\nx = wdbc_df.drop('diagnosis', axis=1).drop('id', axis=1)\ny = np.where(wdbc_df.diagnosis == 'M', 1, 0)\n\nfrom sklearn.model_selection import train_test_split\nf_train , f_test, l_train, l_test = train_test_split(x, y, test_size = 0.3)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "id": "69a686ef",
      "cell_type": "markdown",
      "source": "Now everything should be ready for a few classification trees.  Your\ntask is to analyze the effect of three hyperparameters of DecisionTreeClassifier:\nmax_depth, min_samples_split and min_samples_leaf.  All these hyperparameters help to avoid\noverfitting. \n\n\n4. Explain what do these hyperparameters do.\n\n\n5. Fit a decision tree (on training data), and compute accuracy (on validation data).  Use a combination of all three hyperparameters when defining the model.  As a refresher, you can create it along these lines:\n```\nm = DecisionTreeClassifier(max_depth=7, min_samples_leaf=..., ...)\n```  \nand you can compute accuracy on validation data as\n```\nm.score(Xv, yv)\n```\nwhere Xv and yv are your validation/test features $X$ and test labels $y$. \n",
      "metadata": {}
    },
    {
      "id": "976a4078-0e1b-4804-8df4-ce4c3afc50c1",
      "cell_type": "markdown",
      "source": "Hyperparamters do the following to the decision tree classifier:\n1. max_depth controls the depth of the tree using an integer value. if a value is not provided it will continue until leaf nodes are pure.\n2. min_samples_split is the minimum samples needed to split a node where the default is 2 and any float or int provided will help determine if the node will become a leaf.\n3. min_samples_leaf is the minimum samples needed to be at a leaf node. A split can only happen if there is atleast min_samples_leaf in both of the resulting nodes post split with default of 1",
      "metadata": {}
    },
    {
      "id": "5c076b0d",
      "cell_type": "code",
      "source": "#code goes here\nd_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4)\nm = d_tree.fit(f_test, l_test)\nm.score(f_test, l_test)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 28,
          "output_type": "execute_result",
          "data": {
            "text/plain": "0.9941520467836257"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28
    },
    {
      "id": "d0c26ef7",
      "cell_type": "markdown",
      "source": "Now it is time to do a more thorough search through hyperparameters by\nperforming 3-D grid search.  \n\n6. Write a 3-fold nested loop where the outer loop runs over max depth, next loop runs over min samples split, and the innermost loop runs over min sample leafs.  Use a meaningful set of values for each of these.  For instance, I am using:\n```\ndepths = range(1,6)\nsplits = [2,5,10,20,50,100]\nleafs = [1,2,5,10,20,50,100]\n```\nYou may want to start with a smaller number of combinations to speed\nup the process though.\n\n\nInside of the loop, define a decision tree classifier using these\nparameters, fit it on training data, and compute accuracy on\nvalidation data.  Essentially you repeat question 5, just inside of the loop.\n\n\n6. Find the best accuracy and the corresponding hyperparameter combination your loop can detect.  You can just check inside the innermost loop if the current accuracy is better than the previous best accuracy.\n\n\n7. Finally, compare the best accuracy you achieved using trees with a similar accuracy using logistic regression (on validation data).(You may want to increase max_iter.) Which model gives you better accuracy?\n",
      "metadata": {}
    },
    {
      "id": "65a35a72",
      "cell_type": "code",
      "source": "#code goes here\nbest = 0\nbest_d = 0\nbest_s = 0\nbest_l = 0\nfor depth in range(1,6):\n    for splits in [2,5,10,20,50,100]:\n        for leafs in [1,2,5,10,20,50,100]:\n            d_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = depth, min_samples_split = splits, min_samples_leaf = leafs)\n            m = d_tree.fit(f_test, l_test)\n            \n            score = m.score(f_test, l_test)\n            if score > best:\n                best_s = splits\n                best_d = depth\n                best_l = leafs\n                best = score\nprint('the best model over the given intervals = depth: ' + str(best_d) + ' min_samples_split: ' + str(best_s) + ' min_sample_leafs: ' + str(best_l) + ' score: ' + str(best))",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "the best model over the given intervals = depth: 5 min_samples_split: 2 min_sample_leafs: 1 score: 1.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 36
    },
    {
      "id": "204e45b3-e515-4ff5-ab4c-bc6b895b197c",
      "cell_type": "code",
      "source": "#logistic regression \nimport statsmodels.api as sm\nlog_reg = sm.Logit(l_test, f_test).fit(maxiter = 200)\nlog_reg.summary()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Warning: Maximum number of iterations has been exceeded.\n         Current function value: inf\n         Iterations: 200\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
          "output_type": "stream"
        },
        {
          "execution_count": 47,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                  171\nModel:                          Logit   Df Residuals:                      141\nMethod:                           MLE   Df Model:                           29\nDate:                Wed, 24 Apr 2024   Pseudo R-squ.:                    -inf\nTime:                        14:06:11   Log-Likelihood:                   -inf\nconverged:                      False   LL-Null:                       -113.56\nCovariance Type:            nonrobust   LLR p-value:                     1.000\n=====================================================================================\n                        coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------------\nradius.mean       -1.161e+13        nan        nan        nan         nan         nan\ntexture.mean      -1.394e+13        nan        nan        nan         nan         nan\nperimeter.mean    -7.641e+13        nan        nan        nan         nan         nan\narea.mean         -6.502e+14        nan        nan        nan         nan         nan\nsmoothness.mean   -6.737e+10        nan        nan        nan         nan         nan\ncompactness.mean  -9.036e+10        nan        nan        nan         nan         nan\nconcavity.mean    -1.007e+11        nan        nan        nan         nan         nan\nconcpoints.mean   -5.652e+10        nan        nan        nan         nan         nan\nsymmetry.mean     -1.242e+11        nan        nan        nan         nan         nan\nfracdim.mean      -4.072e+10        nan        nan        nan         nan         nan\nradius.se         -3.738e+11        nan        nan        nan         nan         nan\ntexture.se        -7.279e+11        nan        nan        nan         nan         nan\nperimeter.se      -2.617e+12        nan        nan        nan         nan         nan\narea.se           -4.428e+13        nan        nan        nan         nan         nan\nsmoothness.se     -4.377e+09        nan        nan        nan         nan         nan\ncompactness.se    -1.905e+10        nan        nan        nan         nan         nan\nconcavity.se      -2.535e+10        nan        nan        nan         nan         nan\nconcpoints.se     -9.183e+09        nan        nan        nan         nan         nan\nsymmetry.se        -1.23e+10        nan        nan        nan         nan         nan\nfracdim.se        -2.507e+09        nan        nan        nan         nan         nan\nradius.worst      -1.401e+13        nan        nan        nan         nan         nan\ntexture.worst     -1.881e+13        nan        nan        nan         nan         nan\nperimeter.worst   -9.332e+13        nan        nan        nan         nan         nan\narea.worst        -9.392e+14        nan        nan        nan         nan         nan\nsmoothness.worst  -9.514e+10        nan        nan        nan         nan         nan\ncompactness.worst -2.332e+11        nan        nan        nan         nan         nan\nconcavity.worst   -2.929e+11        nan        nan        nan         nan         nan\nconcpoints.worst  -1.177e+11        nan        nan        nan         nan         nan\nsymmetry.worst    -2.092e+11        nan        nan        nan         nan         nan\nfracdim.worst     -5.958e+10        nan        nan        nan         nan         nan\n=====================================================================================\n\nPossibly complete quasi-separation: A fraction 0.62 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\"\"\"",
            "text/html": "<table class=\"simpletable\">\n<caption>Logit Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>   171</td> \n</tr>\n<tr>\n  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   141</td> \n</tr>\n<tr>\n  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    29</td> \n</tr>\n<tr>\n  <th>Date:</th>            <td>Wed, 24 Apr 2024</td> <th>  Pseudo R-squ.:     </th>  <td>  -inf</td> \n</tr>\n<tr>\n  <th>Time:</th>                <td>14:06:11</td>     <th>  Log-Likelihood:    </th> <td>    -inf</td>\n</tr>\n<tr>\n  <th>converged:</th>             <td>False</td>      <th>  LL-Null:           </th> <td> -113.56</td>\n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td> 1.000</td> \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n          <td></td>             <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>radius.mean</th>       <td>-1.161e+13</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>texture.mean</th>      <td>-1.394e+13</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>perimeter.mean</th>    <td>-7.641e+13</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>area.mean</th>         <td>-6.502e+14</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>smoothness.mean</th>   <td>-6.737e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>compactness.mean</th>  <td>-9.036e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>concavity.mean</th>    <td>-1.007e+11</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>concpoints.mean</th>   <td>-5.652e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>symmetry.mean</th>     <td>-1.242e+11</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>fracdim.mean</th>      <td>-4.072e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>radius.se</th>         <td>-3.738e+11</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>texture.se</th>        <td>-7.279e+11</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>perimeter.se</th>      <td>-2.617e+12</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>area.se</th>           <td>-4.428e+13</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>smoothness.se</th>     <td>-4.377e+09</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>compactness.se</th>    <td>-1.905e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>concavity.se</th>      <td>-2.535e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>concpoints.se</th>     <td>-9.183e+09</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>symmetry.se</th>       <td> -1.23e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>fracdim.se</th>        <td>-2.507e+09</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>radius.worst</th>      <td>-1.401e+13</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>texture.worst</th>     <td>-1.881e+13</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>perimeter.worst</th>   <td>-9.332e+13</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>area.worst</th>        <td>-9.392e+14</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>smoothness.worst</th>  <td>-9.514e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>compactness.worst</th> <td>-2.332e+11</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>concavity.worst</th>   <td>-2.929e+11</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>concpoints.worst</th>  <td>-1.177e+11</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>symmetry.worst</th>    <td>-2.092e+11</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n<tr>\n  <th>fracdim.worst</th>     <td>-5.958e+10</td> <td>      nan</td> <td>      nan</td> <td>   nan</td> <td>      nan</td> <td>      nan</td>\n</tr>\n</table><br/><br/>Possibly complete quasi-separation: A fraction 0.62 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified.",
            "text/latex": "\\begin{center}\n\\begin{tabular}{lclc}\n\\toprule\n\\textbf{Dep. Variable:}    &        y         & \\textbf{  No. Observations:  } &      171    \\\\\n\\textbf{Model:}            &      Logit       & \\textbf{  Df Residuals:      } &      141    \\\\\n\\textbf{Method:}           &       MLE        & \\textbf{  Df Model:          } &       29    \\\\\n\\textbf{Date:}             & Wed, 24 Apr 2024 & \\textbf{  Pseudo R-squ.:     } &     -inf    \\\\\n\\textbf{Time:}             &     14:06:11     & \\textbf{  Log-Likelihood:    } &      -inf   \\\\\n\\textbf{converged:}        &      False       & \\textbf{  LL-Null:           } &   -113.56   \\\\\n\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{  LLR p-value:       } &    1.000    \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tabular}{lcccccc}\n                           & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n\\midrule\n\\textbf{radius.mean}       &   -1.161e+13  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{texture.mean}      &   -1.394e+13  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{perimeter.mean}    &   -7.641e+13  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{area.mean}         &   -6.502e+14  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{smoothness.mean}   &   -6.737e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{compactness.mean}  &   -9.036e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{concavity.mean}    &   -1.007e+11  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{concpoints.mean}   &   -5.652e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{symmetry.mean}     &   -1.242e+11  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{fracdim.mean}      &   -4.072e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{radius.se}         &   -3.738e+11  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{texture.se}        &   -7.279e+11  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{perimeter.se}      &   -2.617e+12  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{area.se}           &   -4.428e+13  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{smoothness.se}     &   -4.377e+09  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{compactness.se}    &   -1.905e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{concavity.se}      &   -2.535e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{concpoints.se}     &   -9.183e+09  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{symmetry.se}       &    -1.23e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{fracdim.se}        &   -2.507e+09  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{radius.worst}      &   -1.401e+13  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{texture.worst}     &   -1.881e+13  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{perimeter.worst}   &   -9.332e+13  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{area.worst}        &   -9.392e+14  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{smoothness.worst}  &   -9.514e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{compactness.worst} &   -2.332e+11  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{concavity.worst}   &   -2.929e+11  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{concpoints.worst}  &   -1.177e+11  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{symmetry.worst}    &   -2.092e+11  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\textbf{fracdim.worst}     &   -5.958e+10  &          nan     &       nan  &           nan        &          nan    &          nan     \\\\\n\\bottomrule\n\\end{tabular}\n%\\caption{Logit Regression Results}\n\\end{center}\n\nPossibly complete quasi-separation: A fraction 0.62 of observations can be \\newline\n perfectly predicted. This might indicate that there is complete \\newline\n quasi-separation. In this case some parameters will not be identified."
          },
          "metadata": {}
        }
      ],
      "execution_count": 47
    },
    {
      "id": "a88dde7f",
      "cell_type": "markdown",
      "source": "## Regression Trees\n\nThis task is a very similar task to the previous one, just you should do a regression, not classification model.  So you can copy-paste most of your code, and then modify it a little bit.\nWe use Boston housing data and predict the median value (medv) using all other\nattributes.  Instead of accuracy, we are now using RMSE, and instead of comparing the result with logistic regression, we compare it with linear regression.\n\n1. Load boston data and ensure it looks good.\n\n\n2. Create your feature matrix $X$ and outcome/label vector $y$.  The former should contain all features, exceot medv, and the latter is medv.\n\n\n3. Split your data into training and validation chunks (or do cross validation below, but that is slower).\n\n4. Fit a regression tree (on training data), and compute RMSE (on validation data).  Use a combination of the same hyperparameterswhen defining the model.  \n  \nAs a refresher, RMSE is defined as\n$RMSE = \\sqrt{\n      \\frac{1}{N} \\sum_{i=1}^{n} (\\hat y_{i} - y_{i})^{2}\n    }$\n\n\n5. Write a similar 3-fold nested loop over these three hyperparameters. Inside of the loop, define a decision tree classifier using these parameters, fit it on training data, and compute RMSE on validation data.  Essentially you repeat question 4, just inside of the loop.\n\n\n6. Find the best accuracy and the corresponding hyperparameter combination your loop can detect.  You can just check inside the innermost loop if the current accuracy is better than the previous best accuracy.\n\n7. Finally, compare the best RMSE you achieved using regression trees with a RMSE of linear regression (on validation data). Which model gives you better accuracy?",
      "metadata": {}
    },
    {
      "id": "27407ed2",
      "cell_type": "code",
      "source": "#code goes here ",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}