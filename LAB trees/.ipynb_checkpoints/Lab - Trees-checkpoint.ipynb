{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36333990",
   "metadata": {},
   "source": [
    "# Lab - Decision Trees \n",
    "\n",
    "This lab asks you to play with regression and classification trees,\n",
    "and find the best combination of hyperparameters.  We use Wisconsin\n",
    "Diagnostic Breast Cancer (WDBC) data for categorization and Boston\n",
    "housing data for regression.  Both tasks are fairly similar.\n",
    "\n",
    "The aim of this lab is to give you some experience with trees and\n",
    "hyperparameter tuning.  Try to get as good accuracy/RMSE as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865efc5",
   "metadata": {},
   "source": [
    "## Classification \n",
    "In this task you work with WDBC data.  As a reminder, your task is to\n",
    "predict __diagnosis__ (''M'' = cancer, ''B'' = no cancer).  \n",
    "\n",
    "\n",
    "1. Load wdbc data and ensure it looks good.\n",
    "\n",
    "\n",
    "2. Create your feature matrix $X$ and label vector $y$.  The former should contain all 30 features,  everything, except __diagnosis__ and __id__.  The latter should be __diagnosis__, converted to either logical or numeric variable (otherwise sklearn will fail).\n",
    "\n",
    "\n",
    "3.  Split your data into training and validation chunks (or do cross validation below, but that is slower).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabd5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code goes here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "wdbc_df = pd.read_csv('wdbc.csv.bz2')\n",
    "x = wdbc_df.drop('diagnosis', axis=1).drop('id', axis=1)\n",
    "y = np.where(wdbc_df.diagnosis == 'M', 1, 0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "f_train , f_test, l_train, l_test = train_test_split(x, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a686ef",
   "metadata": {},
   "source": [
    "Now everything should be ready for a few classification trees.  Your\n",
    "task is to analyze the effect of three hyperparameters of DecisionTreeClassifier:\n",
    "max_depth, min_samples_split and min_samples_leaf.  All these hyperparameters help to avoid\n",
    "overfitting. \n",
    "\n",
    "\n",
    "4. Explain what do these hyperparameters do.\n",
    "\n",
    "\n",
    "5. Fit a decision tree (on training data), and compute accuracy (on validation data).  Use a combination of all three hyperparameters when defining the model.  As a refresher, you can create it along these lines:\n",
    "```\n",
    "m = DecisionTreeClassifier(max_depth=7, min_samples_leaf=..., ...)\n",
    "```  \n",
    "and you can compute accuracy on validation data as\n",
    "```\n",
    "m.score(Xv, yv)\n",
    "```\n",
    "where Xv and yv are your validation/test features $X$ and test labels $y$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a4078-0e1b-4804-8df4-ce4c3afc50c1",
   "metadata": {},
   "source": [
    "Hyperparamters do the following to the decision tree classifier:\n",
    "1. max_depth controls the depth of the tree using an integer value. if a value is not provided it will continue until leaf nodes are pure.\n",
    "2. min_samples_split is the minimum samples needed to split a node where the default is 2 and any float or int provided will help determine if the node will become a leaf.\n",
    "3. min_samples_leaf is the minimum samples needed to be at a leaf node. A split can only happen if there is atleast min_samples_leaf in both of the resulting nodes post split with default of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c076b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9064327485380117"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code goes here\n",
    "d_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 4)\n",
    "m = d_tree.fit(f_train, l_train)\n",
    "m.score(f_test, l_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c26ef7",
   "metadata": {},
   "source": [
    "Now it is time to do a more thorough search through hyperparameters by\n",
    "performing 3-D grid search.  \n",
    "\n",
    "6. Write a 3-fold nested loop where the outer loop runs over max depth, next loop runs over min samples split, and the innermost loop runs over min sample leafs.  Use a meaningful set of values for each of these.  For instance, I am using:\n",
    "```\n",
    "depths = range(1,6)\n",
    "splits = [2,5,10,20,50,100]\n",
    "leafs = [1,2,5,10,20,50,100]\n",
    "```\n",
    "You may want to start with a smaller number of combinations to speed\n",
    "up the process though.\n",
    "\n",
    "\n",
    "Inside of the loop, define a decision tree classifier using these\n",
    "parameters, fit it on training data, and compute accuracy on\n",
    "validation data.  Essentially you repeat question 5, just inside of the loop.\n",
    "\n",
    "\n",
    "6. Find the best accuracy and the corresponding hyperparameter combination your loop can detect.  You can just check inside the innermost loop if the current accuracy is better than the previous best accuracy.\n",
    "\n",
    "\n",
    "7. Finally, compare the best accuracy you achieved using trees with a similar accuracy using logistic regression (on validation data).(You may want to increase max_iter.) Which model gives you better accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65a35a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best model over the given intervals = depth: 8 min_samples_split: 2 min_sample_leafs: 1 score: 0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "#code goes here\n",
    "best = 0\n",
    "best_d = 0\n",
    "best_s = 0\n",
    "best_l = 0\n",
    "for depth in range(1,10):\n",
    "    for splits in [2,5,10,20,50,100]:\n",
    "        for leafs in [1,2,5,10,20,50,100]:\n",
    "            d_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = depth, min_samples_split = splits, min_samples_leaf = leafs)\n",
    "            m = d_tree.fit(f_train, l_train)\n",
    "            \n",
    "            score = m.score(f_test, l_test)\n",
    "            if score > best:\n",
    "                best_s = splits\n",
    "                best_d = depth\n",
    "                best_l = leafs\n",
    "                best = score\n",
    "print('the best model over the given intervals = depth: ' + str(best_d) + ' min_samples_split: ' + str(best_s) + ' min_sample_leafs: ' + str(best_l) + ' score: ' + str(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204e45b3-e515-4ff5-ab4c-bc6b895b197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "log_reg = LogisticRegression(max_iter=10000)  # Increase max_iter\n",
    "log_reg.fit(f_train, l_train)\n",
    "val_acc_log_reg = accuracy_score(l_test, log_reg.predict(f_test))\n",
    "val_acc_log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109a924-94f5-4c06-9148-41ca17d5654b",
   "metadata": {},
   "source": [
    "The results show that the paramaterized tree is much more accurate than its standard counterpart but fails to beat the logistic regression in this case. The only exception is that the logistic regression requires extreme max iteration values to stabilize which is computationally innefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88dde7f",
   "metadata": {},
   "source": [
    "## Regression Trees\n",
    "\n",
    "This task is a very similar task to the previous one, just you should do a regression, not classification model.  So you can copy-paste most of your code, and then modify it a little bit.\n",
    "We use Boston housing data and predict the median value (medv) using all other\n",
    "attributes.  Instead of accuracy, we are now using RMSE, and instead of comparing the result with logistic regression, we compare it with linear regression.\n",
    "\n",
    "1. Load boston data and ensure it looks good.\n",
    "\n",
    "\n",
    "2. Create your feature matrix $X$ and outcome/label vector $y$.  The former should contain all features, exceot medv, and the latter is medv.\n",
    "\n",
    "\n",
    "3. Split your data into training and validation chunks (or do cross validation below, but that is slower).\n",
    "\n",
    "4. Fit a regression tree (on training data), and compute RMSE (on validation data).  Use a combination of the same hyperparameterswhen defining the model.  \n",
    "  \n",
    "As a refresher, RMSE is defined as\n",
    "$RMSE = \\sqrt{\n",
    "      \\frac{1}{N} \\sum_{i=1}^{n} (\\hat y_{i} - y_{i})^{2}\n",
    "    }$\n",
    "\n",
    "\n",
    "5. Write a similar 3-fold nested loop over these three hyperparameters. Inside of the loop, define a decision tree classifier using these parameters, fit it on training data, and compute RMSE on validation data.  Essentially you repeat question 4, just inside of the loop.\n",
    "\n",
    "\n",
    "6. Find the best accuracy and the corresponding hyperparameter combination your loop can detect.  You can just check inside the innermost loop if the current accuracy is better than the previous best accuracy.\n",
    "\n",
    "7. Finally, compare the best RMSE you achieved using regression trees with a RMSE of linear regression (on validation data). Which model gives you better accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27407ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score with no hyperparams 3.7270437699546672\n",
      "the best model over the given intervals = depth: 7 min_samples_split: 20 min_sample_leafs: 1 score: 3.2226297351806554\n",
      "Linear Regression RMSE: 4.513073584732745\n"
     ]
    }
   ],
   "source": [
    "#code goes here  \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "boston = pd.read_csv('boston.csv.bz2', delimiter='\\t')\n",
    "x = boston.drop('medv', axis=1)\n",
    "y = boston.medv\n",
    "f_train , f_test, l_train, l_test = train_test_split(x, y, test_size = 0.3)\n",
    "d_tree = tree.DecisionTreeRegressor(criterion = 'squared_error')\n",
    "m = d_tree.fit(f_train, l_train)\n",
    "y_pred = m.predict(f_test)\n",
    "score = mean_squared_error(l_test, y_pred, squared=False)\n",
    "print(\"Test score with no hyperparams\", score)\n",
    "\n",
    "best = 10000\n",
    "best_d = 0\n",
    "best_s = 0\n",
    "best_l = 0\n",
    "for depth in range(1,10):\n",
    "    for splits in [2,5,10,20,50,100]:\n",
    "        for leafs in [1,2,5,10,20,50,100]:\n",
    "            d_tree = tree.DecisionTreeRegressor(criterion = 'squared_error', max_depth = depth, min_samples_split = splits, min_samples_leaf = leafs)\n",
    "            m = d_tree.fit(f_train, l_train)\n",
    "            y_pred = m.predict(f_test)\n",
    "            score = mean_squared_error(l_test, y_pred, squared=False)\n",
    "            if score < best:\n",
    "                best_s = splits\n",
    "                best_d = depth\n",
    "                best_l = leafs\n",
    "                best = score\n",
    "print('the best model over the given intervals = depth: ' + str(best_d) + ' min_samples_split: ' + str(best_s) + ' min_sample_leafs: ' + str(best_l) + ' score: ' + str(best))\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(f_train, l_train)\n",
    "y_pred_linear = linear_reg.predict(f_test)\n",
    "score_linear = mean_squared_error(l_test, y_pred_linear, squared=False)\n",
    "print(\"Linear Regression RMSE:\", score_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2959c2",
   "metadata": {},
   "source": [
    "The results of the tree regression indicate that the lowest squared error of the system is 3.22 with hyperparams of depth=7, min_sample_split=20 and min_sample_leafs=1. The linear regression returns an RMSE of 4.51 which is signifcantly worse than the paramterized tree showing how trees can more accurately approximate functions over regression lines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
