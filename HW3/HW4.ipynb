{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0902fdb",
   "metadata": {},
   "source": [
    "# INFO371 Problem Set: Bayes-Theorem based Spam Filter\n",
    "\n",
    "In this problem set you will use Bayes Theorem to categorize \n",
    "emails from Ling-Spam corpus into spam and non-spam.  Using a single-word-based Bayes approach does not give good results, but this problem set serves as a preparatory\n",
    "work for understanding the Naive Bayes approach.\n",
    "\n",
    "\n",
    "## Ling-Spam emails\n",
    "\n",
    "The corpus contains ~ 2700 emails from academic accounts talking\n",
    "about conferences, deadlines, papers etc, and peppered with wonderful\n",
    "offers of viagra, lottery millions and similar spam messages.  The\n",
    "emails have been converted into a csv file that contains three variables:\n",
    "\n",
    "* spam --> true or false, this email is spam\n",
    "* files --> the original file name for this email (not needed in this HW).\n",
    "* message --> the content of the email in a single line\n",
    "\n",
    "\n",
    "## (5pt) Explore and clean the data\n",
    "\n",
    "First, let's load data and take a closer look at it.\n",
    "\n",
    "1. (2pt) Load the lingspam-emails.csv.bz2 dataset.  Browse a handful of emails, both spam and non-spam ones, to see what kind of text we are working with here.Hint: check out textwrap module to print long strings on multiple lines.\n",
    "  \n",
    "  \n",
    "2. (3pt) Ensure the data is clean: remove all cases with missing spam and empty message field.  We do not care about the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ec4275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>Subject: re : 2 . 882 s - &gt; np np  &gt; date : su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>Subject: s - &gt; np + np  the discussion of s - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>Subject: 2 . 882 s - &gt; np np  . . . for me it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>Subject: gent conference  \" for the listserv \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>Subject: query : causatives in korean  could a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    spam                                            message\n",
       "0  False  Subject: re : 2 . 882 s - > np np  > date : su...\n",
       "1  False  Subject: s - > np + np  the discussion of s - ...\n",
       "2  False  Subject: 2 . 882 s - > np np  . . . for me it ...\n",
       "3  False  Subject: gent conference  \" for the listserv \"...\n",
       "4  False  Subject: query : causatives in korean  could a..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code goes here \n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"lingspam-emails.csv.bz2\", delimiter = '\\t')\n",
    "df = df.dropna(subset=['spam', 'message'])\n",
    "df = df[['spam', 'message']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584bbfa",
   "metadata": {},
   "source": [
    "## (15pt) Create Document-term matrix (DTM)\n",
    "\n",
    "The first serious step is to create the document-term matrix (DTM).\n",
    "This is simply numeric indicators for selected words: does this email\n",
    "contain the word (1) or not (0).  But before we get there, we have to\n",
    "decide the words.\n",
    "\n",
    "\n",
    "1. (2pt) Choose 10+ words which might be good to distinguish between spam/non-spam.  Use these four: ''viagra'', ''deadline'', ''million'', and ''and''.  Choose more words yourself (you may want to return here and reconsider your choice later).\n",
    "\n",
    "\n",
    "2. (10pt) Convert your messages into DTM.  We do not use the full 60k-words DTM here but only a baby-DTM of the 10 words you picked above. You may add the DTM columns to the original data frame, or keep those in a separate structure. \n",
    "\n",
    "Creating the DTM involves finding whether the word is contained in the message for all emails in data. You can loop over emails and check each one individually, but pandas string methods make life much easier.  You will want to do case-insensitive matching, checking for both upper and lower case.  You may consider something like this:\n",
    "\n",
    "```\n",
    "for w in list_of_words:\n",
    "    emails[w] = emails.message.str.lower().str.contains(w)\n",
    "```\n",
    "\n",
    "  Note: It is more intuitive to work with your data if you\n",
    "  convert the logical values returned by contains to numbers.\n",
    "  \n",
    "  \n",
    "  \n",
    "3. (3pt) Split your work data (i.e. the DTM) and target (the spam indicator) into training and validation chunks (80/20 is a good split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2faac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   viagra  deadline  million  and  fast  crypto  click  now  money  free\n",
      "0       0         0        0    1     0       0      0    1      0     0\n",
      "1       0         0        0    0     0       0      0    1      0     0\n",
      "2       0         0        0    0     0       0      0    0      0     0\n",
      "3       0         0        0    1     0       0      0    0      1     0\n",
      "4       0         0        0    1     0       0      0    0      0     0\n"
     ]
    }
   ],
   "source": [
    "# code goes here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "words = ['viagra', 'deadline', \"million\",\"and\", \"fast\", \"crypto\", 'click', \"now\", 'money', 'free']\n",
    "model = CountVectorizer(vocabulary=words, binary=True)\n",
    "dtm = model.fit_transform(df.message.str.lower())\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), columns=words)\n",
    "print(dtm_df.head())\n",
    "X = dtm_df\n",
    "y = df.spam\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b469941",
   "metadata": {},
   "source": [
    "## (80pt) Estimate and validate\n",
    "\n",
    "Now you are ready with the preparatory work and it's time to\n",
    "dive into the real thing.  Let's rehearse the Bayes theorem here\n",
    "again.  We want to estimate the probability that an email is spam, given it\n",
    "contains a certain word: \n",
    "\n",
    "$Pr(category = S|w = 1) = \\frac{Pr(w=1|category = S) * Pr(category=S)}{Pr(w=1)}$.\n",
    "\n",
    "\n",
    "In order to compute this probability, we need to calculate some other\n",
    "probabilities: \n",
    "\n",
    "* $Pr(category=S)$ --> Probability of spam in data\n",
    "\n",
    "* $Pr(category=NS)$ --> Probablility for non-spam in data\n",
    "\n",
    "* $Pr(w=1)$ --> Probability the word is seen in messages\n",
    "\n",
    "* $Pr(w=0)$ --> probability the word is not seen in messages\n",
    "\n",
    "* $Pr(w=1|category = S)$ --> & probability the word is seen in messages that are spam\n",
    "\n",
    "* $Pr(w=1|category = NS)$ --> probability the word is seen in messages that are not spam\n",
    "\n",
    "....\n",
    "\n",
    "\n",
    "but it turns out we are still not done with preparations. Namely, you need to compute \n",
    "quite a few different probabilities below, including $Pr(category=S)$, $Pr(category=NS)$, $Pr(w=1)$, $Pr(w=0)$, $Pr(w=1|category = S)$, $Pr(w=0|category = S)$, $Pr(w=1|category = NS)$, $Pr(w=0|category = NS)$.\n",
    "\n",
    "\n",
    "1. (2pt) Design a scheme for your variable names that describes these probabilities so that a) you understand what they mean; and b) the others (including your grader) will understand those! Hint: you may get some ideas from the [Python notes](https://faculty.washington.edu/otoomet/machinelearning-py/python.html#base-language) in Section 2.3, Base Language.\n",
    "\n",
    "The first task is to compute these probabilities.\n",
    "Use only training data for this task.\n",
    "\n",
    "2. (4pt) Compute the priors, the unconditional probabilities for an email being spam and non-spam, $Pr(category=S)$ and $Pr(category=NS)$.  These probabilities are based on the spam variable alone, not on the text.\n",
    "\n",
    "\n",
    "The next tasks involve computing the following probabilities for each\n",
    "word out of the list of 10 you picked above,\n",
    "I recommend to avoid unneccessary complexity and\n",
    "just to write a loop over the words, compute the\n",
    "answers, and print the word and the corresponding results there.  \n",
    "\n",
    "\n",
    "\n",
    "3. (4pt) For each word $w$, compute the normalizers, $Pr(w=1)$ and $Pr(w=0)$.\n",
    "  \n",
    "  Hint: this is $Pr(million = 1) = 0.0484$.  But note this value\n",
    "  (and the following hints) depends on your random training/validation split!\n",
    "  \n",
    "  \n",
    "4. (7pt) For each word $w$, compute $Pr(w=1|category = S)$ and $Pr(w=1|category = NS)$.  These probabilities are based on both the spam-variable and on the DTM component that corresponds to the word $w$.\n",
    "  \n",
    "  Hint: $Pr(million = 1|category = S) = 0.252$\n",
    "  \n",
    "  \n",
    "5. (5pt) Finally, compute the probabilities of interest, $Pr(category = S|w = 1)$ and $Pr(category = S|w = 0)$.  Compute this value using Bayes theorem, not directly by counting! \n",
    "  \n",
    "  For the check, you may also compute\n",
    "  $Pr(category = NS|w = 1)$ and $Pr(category = NS|w = 0)$\n",
    "  \n",
    "  Hint: $\\Pr(\\mathit{category} = S|\\mathit{million} = 1) = 0.843$.  But\n",
    "  note this number depends on your random testing-validation split!\n",
    "\n",
    "\n",
    "6. (6pt)  Which of these probabilities have to sum to one? (E.g. $Pr(category = 1) + Pr(category = 0) = 1$.) Which ones do not?  Explain!\n",
    "\n",
    "---\n",
    "Now we are done with the estimator.  Your fitted model is completely\n",
    "described by these probabilities.  Let's now turn to prediction, using\n",
    "your validation data.  Note that we are still inside the loop over\n",
    "each word $w$!\n",
    "\n",
    "9. (8pt) For each email in your validation set, predict whether it is predicted to be spam or non-spam.  Hint: you should check if it contains the word $w$ and use the appropriate probability, $Pr(category = S|w = 1)$ or $Pr(category = S|w = 0)$.\n",
    "\n",
    "\n",
    "10. (5pt) Print the resulting confusion matrix and compute accuracy, precision and recall.\n",
    "\n",
    "\n",
    "11. (5pt) Which steps above constitute model training?  In which steps do you use trained model?  What is a trained model in this case? Explain! \n",
    "  \n",
    "  Hint: a trained model is all you need to make predictions.\n",
    "\n",
    "---\n",
    "Now it is time to look at your results a little bit closer.\n",
    "\n",
    "12. (4pt) Comment the overall performance of the model--how do accuracy, precision and recall look like?\n",
    "\n",
    "\n",
    "13. (8pt) Explain why do you see very low recall while the other indicators do not look that bad.\n",
    "\n",
    "\n",
    "14. (8pt) Explain why some words work well and others not: \n",
    "  * why does ''million'' improve accuracy?\n",
    "  * why does ''viagra'' not work?\n",
    "  * why does ''deadline'' not work?\n",
    "  * why does ''and'' not work?\n",
    "\n",
    "  Hint: You may just see where in which emails these words occur, and\n",
    "  how frequently.  These are all different reasons!\n",
    "  \n",
    "---\n",
    "Finally, let's add Laplace smoothing to this model.  One can imagine\n",
    "Laplace smoothing as two additional ''ghost'' observations, one spam\n",
    "and one non-spam.  Both of these ghost observations contain every\n",
    "single word in our DTM.  See also [Lecture Notes](https://faculty.washington.edu/otoomet/machineLearning.pdf), Ch 7.3.2 ''Smoothing: how to compute probabilities with too\n",
    "few data'', page 263.\n",
    "\n",
    "Laplace smoothing does not add anything here but it is is a crucial\n",
    "tool when we move to Naive Bayes later.\n",
    "\n",
    "15. (5pt) Add such smoothing to the model.  You can either literally add two such lines of data, or alternatively manipulate the way you compute the probabilities.\n",
    "\n",
    "\n",
    "16. (5pt) Repeat the tasks above: compute the probabilities, do predictions, compute the accuracy, precision, recall for all words.  \n",
    "\n",
    "\n",
    "17. (4pt) Comment on the results.  Does smoothing improve the overall performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ec7aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(category = S) and P(category = NS)\n",
      "0.16810717372515124 0.8318928262748487\n",
      "\n",
      "Probabilities of w=1 for each word in the set\n",
      "viagra      0.000432\n",
      "deadline    0.143042\n",
      "million     0.034572\n",
      "and         0.923077\n",
      "fast        0.023768\n",
      "crypto      0.001729\n",
      "click       0.049697\n",
      "now         0.212187\n",
      "money       0.085566\n",
      "free        0.154278\n",
      "dtype: float64\n",
      "\n",
      "Probabilities of w=0 for each word in the set\n",
      "viagra      0.999568\n",
      "deadline    0.856958\n",
      "million     0.965428\n",
      "and         0.076923\n",
      "fast        0.976232\n",
      "crypto      0.998271\n",
      "click       0.950303\n",
      "now         0.787813\n",
      "money       0.914434\n",
      "free        0.845722\n",
      "dtype: float64\n",
      "\n",
      "Probabilities of w=1 for each word in the set given spam = True\n",
      "viagra      0.002571\n",
      "deadline    0.000000\n",
      "million     0.161954\n",
      "and         0.902314\n",
      "fast        0.079692\n",
      "crypto      0.005141\n",
      "click       0.264781\n",
      "now         0.503856\n",
      "money       0.370180\n",
      "free        0.570694\n",
      "dtype: float64\n",
      "\n",
      "Probabilities of w=1 for each word in the set given spam = False\n",
      "viagra      0.000000\n",
      "deadline    0.171948\n",
      "million     0.008831\n",
      "and         0.927273\n",
      "fast        0.012468\n",
      "crypto      0.001039\n",
      "click       0.006234\n",
      "now         0.153247\n",
      "money       0.028052\n",
      "free        0.070130\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "probability_spam = y_train.sum() / len(y_train)\n",
    "probability_not_spam = 1 - probability_spam\n",
    "print(\"P(category = S) and P(category = NS)\")\n",
    "print(probability_spam, probability_not_spam)\n",
    "print()\n",
    "\n",
    "print('Probabilities of w=1 for each word in the set')\n",
    "print(X_train.mean())\n",
    "print()\n",
    "print('Probabilities of w=0 for each word in the set')\n",
    "print(1-X_train.mean())\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "spam_data = train_data[train_data['spam'] == True]\n",
    "non_spam_data = train_data[train_data['spam'] == False]\n",
    "X_train_spam = spam_data.drop(columns='spam')\n",
    "X_train_not_spam = non_spam_data.drop(columns='spam')\n",
    "\n",
    "print('Probabilities of w=1 for each word in the set given spam = True')\n",
    "print(X_train_spam.mean())\n",
    "print()\n",
    "print('Probabilities of w=1 for each word in the set given spam = False')\n",
    "print(X_train_not_spam.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4df335fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of category=S given word=1:\n",
      "viagra      1.000000\n",
      "deadline    0.000000\n",
      "million     0.787500\n",
      "and         0.164326\n",
      "fast        0.563636\n",
      "crypto      0.500000\n",
      "click       0.895652\n",
      "now         0.399185\n",
      "money       0.727273\n",
      "free        0.621849\n",
      "dtype: float64\n",
      "\n",
      "Probability of category=S given word=0:\n",
      "viagra      0.167748\n",
      "deadline    0.196167\n",
      "million     0.145927\n",
      "and         0.213483\n",
      "fast        0.158477\n",
      "crypto      0.167532\n",
      "click       0.130059\n",
      "now         0.105869\n",
      "money       0.115784\n",
      "free        0.085335\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "P_category_spam = probability_spam\n",
    "P_category_not_spam = probability_not_spam\n",
    "P_word_1 = X_train.mean()\n",
    "P_word_0 = 1 - P_word_1\n",
    "\n",
    "#bayes theorem\n",
    "P_category_spam_given_word_1 = (X_train_spam.mean() * P_category_spam) / P_word_1\n",
    "P_category_spam_given_word_0 = ((1 - X_train_spam.mean()) * P_category_spam) / P_word_0\n",
    "\n",
    "print('Probability of category=S given word=1:')\n",
    "print(P_category_spam_given_word_1)\n",
    "print()\n",
    "\n",
    "print('Probability of category=S given word=0:')\n",
    "print(P_category_spam_given_word_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ca004",
   "metadata": {},
   "source": [
    "The probabilities which have to sum to 1 are the ones which are compliments of each other. For example all priors such as P(category = S) and P(category = NS) must be one as spam + non-spam is the whole set. The addition of conditionallity means that generally most of the remaining probabilities do not sum to one as they both do represent partials of a whole but instead subsets of a general domain (depending on the condition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f8e97c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model:  0.9050086355785838\n",
      "Recall of our model:  0.4673913043478261\n",
      "Precision of our model:  0.8775510204081632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x214cd3aba60>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, recall_score, precision_score\n",
    "prediction_array = []\n",
    "\n",
    "for index, row in X_test.iterrows():\n",
    "    contains_word = any(value == 1 for value in row)\n",
    "    if contains_word:\n",
    "        prediction = any(P_category_spam_given_word_1[word] > 0.75 for word, value in row.items() if value == 1)\n",
    "    else:\n",
    "        prediction = any(P_category_spam_given_word_0[word] > 0.75 for word, value in row.items() if value == 0)\n",
    "    prediction_array.append(prediction)\n",
    "    \n",
    "print('Accuracy of our model: ',accuracy_score(y_test,prediction_array))\n",
    "print('Recall of our model: ',recall_score(y_test,prediction_array))\n",
    "print('Precision of our model: ',precision_score(y_test,prediction_array))\n",
    "\n",
    "conf = confusion_matrix(y_test,prediction_array)\n",
    "disp = ConfusionMatrixDisplay(conf)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62777f",
   "metadata": {},
   "source": [
    "The results above show a relatively well trained model which can predict spam at an accuracy of around 90% based on the training data and bayes theorem.\n",
    "\n",
    "Which steps above constitute model training? In which steps do you use trained model? What is a trained model in this case? Explain!\n",
    "\n",
    "Model training is the portion of forming the Probability series for each word. Doing this allows up to create a map of out data which tells us if a word w exists whats the probability it is spam and gives \"direction\" to how a predication should be made steps 4 and 5 are the closest to model training. The trained model in this case is the probability map as it tells us how to make predictive decisions probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2aa911",
   "metadata": {},
   "source": [
    "\n",
    "The performance of the model is quite strong with an accuracy of 90% a precision of 88% and a recall of 47%.\n",
    "\n",
    "A low recall indicates that while the model guess well overall (accuracy) and when it guesses it is correct (precision) it is not accuretly capturing all of the spam emails in the filter. The reason for this is probably the small number of words used in the set.\n",
    "\n",
    "Million works well because it generally indicates spam if its present in a high probability. Viagra and deadline both fail for the same reason as they have probabilities of 1 and 0 respectively which in the case of bayes theorem is problematic for calculations. Generally probabilities of 0 and 1 require smoothing to be leveraged effectively in a NB model. Additionally there may be extremely low volume but high probability for certain words which result in columns the model deems as entirely important and definuing. And doesn't work simply due to its abudence. In the conditional probabilities it is clear that the presence or lack of presence of and does not really give information as almost all emails contained it.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82743df",
   "metadata": {},
   "source": [
    "Notes for the assignment:\n",
    "\n",
    "1. I felt like this HW was significantly more difficult and arduous than the in class examples. I found myself having to take the general approach we learned using CategoricalNB and back-engineering it with lab 3 to try and formulate this assignment. If we could spend more time doing the \"by-hand in code\" examples and less SKlearn hand holding I would have felt way more prepared for this HW. THANK YOU!\n",
    "2. TIME TO COMPLETE: approximately 10-12 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d69c20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
