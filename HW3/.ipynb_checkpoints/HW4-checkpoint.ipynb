{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0902fdb",
   "metadata": {},
   "source": [
    "# INFO371 Problem Set: Bayes-Theorem based Spam Filter\n",
    "\n",
    "In this problem set you will use Bayes Theorem to categorize \n",
    "emails from Ling-Spam corpus into spam and non-spam.  Using a single-word-based Bayes approach does not give good results, but this problem set serves as a preparatory\n",
    "work for understanding the Naive Bayes approach.\n",
    "\n",
    "\n",
    "## Ling-Spam emails\n",
    "\n",
    "The corpus contains ~ 2700 emails from academic accounts talking\n",
    "about conferences, deadlines, papers etc, and peppered with wonderful\n",
    "offers of viagra, lottery millions and similar spam messages.  The\n",
    "emails have been converted into a csv file that contains three variables:\n",
    "\n",
    "* spam --> true or false, this email is spam\n",
    "* files --> the original file name for this email (not needed in this HW).\n",
    "* message --> the content of the email in a single line\n",
    "\n",
    "\n",
    "## (5pt) Explore and clean the data\n",
    "\n",
    "First, let's load data and take a closer look at it.\n",
    "\n",
    "1. (2pt) Load the lingspam-emails.csv.bz2 dataset.  Browse a handful of emails, both spam and non-spam ones, to see what kind of text we are working with here.Hint: check out textwrap module to print long strings on multiple lines.\n",
    "  \n",
    "  \n",
    "2. (3pt) Ensure the data is clean: remove all cases with missing spam and empty message field.  We do not care about the file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec4275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here \n",
    "import opa\n",
    "df = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584bbfa",
   "metadata": {},
   "source": [
    "## (15pt) Create Document-term matrix (DTM)\n",
    "\n",
    "The first serious step is to create the document-term matrix (DTM).\n",
    "This is simply numeric indicators for selected words: does this email\n",
    "contain the word (1) or not (0).  But before we get there, we have to\n",
    "decide the words.\n",
    "\n",
    "\n",
    "1. (2pt) Choose 10+ words which might be good to distinguish between spam/non-spam.  Use these four: ''viagra'', ''deadline'', ''million'', and ''and''.  Choose more words yourself (you may want to return here and reconsider your choice later).\n",
    "\n",
    "\n",
    "2. (10pt) Convert your messages into DTM.  We do not use the full 60k-words DTM here but only a baby-DTM of the 10 words you picked above. You may add the DTM columns to the original data frame, or keep those in a separate structure. \n",
    "\n",
    "Creating the DTM involves finding whether the word is contained in the message for all emails in data. You can loop over emails and check each one individually, but pandas string methods make life much easier.  You will want to do case-insensitive matching, checking for both upper and lower case.  You may consider something like this:\n",
    "\n",
    "```\n",
    "for w in list_of_words:\n",
    "    emails[w] = emails.message.str.lower().str.contains(w)\n",
    "```\n",
    "\n",
    "  Note: It is more intuitive to work with your data if you\n",
    "  convert the logical values returned by contains to numbers.\n",
    "  \n",
    "  \n",
    "  \n",
    "3. (3pt) Split your work data (i.e. the DTM) and target (the spam indicator) into training and validation chunks (80/20 is a good split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2faac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b469941",
   "metadata": {},
   "source": [
    "## (80pt) Estimate and validate\n",
    "\n",
    "Now you are ready with the preparatory work and it's time to\n",
    "dive into the real thing.  Let's rehearse the Bayes theorem here\n",
    "again.  We want to estimate the probability that an email is spam, given it\n",
    "contains a certain word: \n",
    "\n",
    "$Pr(category = S|w = 1) = \\frac{Pr(w=1|category = S) * Pr(category=S)}{Pr(w=1)}$.\n",
    "\n",
    "\n",
    "In order to compute this probability, we need to calculate some other\n",
    "probabilities: \n",
    "\n",
    "* $Pr(category=S)$ --> Probability of spam in data\n",
    "\n",
    "* $Pr(category=NS)$ --> Probablility for non-spam in data\n",
    "\n",
    "* $Pr(w=1)$ --> Probability the word is seen in messages\n",
    "\n",
    "* $Pr(w=0)$ --> probability the word is not seen in messages\n",
    "\n",
    "* $Pr(w=1|category = S)$ --> & probability the word is seen in messages that are spam\n",
    "\n",
    "* $Pr(w=1|category = NS)$ --> probability the word is seen in messages that are not spam\n",
    "\n",
    "....\n",
    "\n",
    "\n",
    "but it turns out we are still not done with preparations. Namely, you need to compute \n",
    "quite a few different probabilities below, including $Pr(category=S)$, $Pr(category=NS)$, $Pr(w=1)$, $Pr(w=0)$, $Pr(w=1|category = S)$, $Pr(w=0|category = S)$, $Pr(w=1|category = NS)$, $Pr(w=0|category = NS)$.\n",
    "\n",
    "\n",
    "1. (2pt) Design a scheme for your variable names that describes these probabilities so that a) you understand what they mean; and b) the others (including your grader) will understand those! Hint: you may get some ideas from the [Python notes](https://faculty.washington.edu/otoomet/machinelearning-py/python.html#base-language) in Section 2.3, Base Language.\n",
    "\n",
    "The first task is to compute these probabilities.\n",
    "Use only training data for this task.\n",
    "\n",
    "2. (4pt) Compute the priors, the unconditional probabilities for an email being spam and non-spam, $Pr(category=S)$ and $Pr(category=NS)$.  These probabilities are based on the spam variable alone, not on the text.\n",
    "\n",
    "\n",
    "The next tasks involve computing the following probabilities for each\n",
    "word out of the list of 10 you picked above,\n",
    "I recommend to avoid unneccessary complexity and\n",
    "just to write a loop over the words, compute the\n",
    "answers, and print the word and the corresponding results there.  \n",
    "\n",
    "\n",
    "\n",
    "3. (4pt) For each word $w$, compute the normalizers, $Pr(w=1)$ and $Pr(w=0)$.\n",
    "  \n",
    "  Hint: this is $Pr(million = 1) = 0.0484$.  But note this value\n",
    "  (and the following hints) depends on your random training/validation split!\n",
    "  \n",
    "  \n",
    "4. (7pt) For each word $w$, compute $Pr(w=1|category = S)$ and $Pr(w=1|category = NS)$.  These probabilities are based on both the spam-variable and on the DTM component that corresponds to the word $w$.\n",
    "  \n",
    "  Hint: $Pr(million = 1|category = S) = 0.252$\n",
    "  \n",
    "  \n",
    "5. (5pt) Finally, compute the probabilities of interest, $Pr(category = S|w = 1)$ and $Pr(category = S|w = 0)$.  Compute this value using Bayes theorem, not directly by counting! \n",
    "  \n",
    "  For the check, you may also compute\n",
    "  $Pr(category = NS|w = 1)$ and $Pr(category = NS|w = 0)$\n",
    "  \n",
    "  Hint: $\\Pr(\\mathit{category} = S|\\mathit{million} = 1) = 0.843$.  But\n",
    "  note this number depends on your random testing-validation split!\n",
    "\n",
    "\n",
    "6. (6pt)  Which of these probabilities have to sum to one? (E.g. $Pr(category = 1) + Pr(category = 0) = 1$.) Which ones do not?  Explain!\n",
    "\n",
    "---\n",
    "Now we are done with the estimator.  Your fitted model is completely\n",
    "described by these probabilities.  Let's now turn to prediction, using\n",
    "your validation data.  Note that we are still inside the loop over\n",
    "each word $w$!\n",
    "\n",
    "9. (8pt) For each email in your validation set, predict whether it is predicted to be spam or non-spam.  Hint: you should check if it contains the word $w$ and use the appropriate probability, $Pr(category = S|w = 1)$ or $Pr(category = S|w = 0)$.\n",
    "\n",
    "\n",
    "10. (5pt) Print the resulting confusion matrix and compute accuracy, precision and recall.\n",
    "\n",
    "\n",
    "11. (5pt) Which steps above constitute model training?  In which steps do you use trained model?  What is a trained model in this case? Explain! \n",
    "  \n",
    "  Hint: a trained model is all you need to make predictions.\n",
    "\n",
    "---\n",
    "Now it is time to look at your results a little bit closer.\n",
    "\n",
    "12. (4pt) Comment the overall performance of the model--how do accuracy, precision and recall look like?\n",
    "\n",
    "\n",
    "13. (8pt) Explain why do you see very low recall while the other indicators do not look that bad.\n",
    "\n",
    "\n",
    "14. (8pt) Explain why some words work well and others not: \n",
    "  * why does ''million'' improve accuracy?\n",
    "  * why does ''viagra'' not work?\n",
    "  * why does ''deadline'' not work?\n",
    "  * why does ''and'' not work?\n",
    "\n",
    "  Hint: You may just see where in which emails these words occur, and\n",
    "  how frequently.  These are all different reasons!\n",
    "  \n",
    "---\n",
    "Finally, let's add Laplace smoothing to this model.  One can imagine\n",
    "Laplace smoothing as two additional ''ghost'' observations, one spam\n",
    "and one non-spam.  Both of these ghost observations contain every\n",
    "single word in our DTM.  See also [Lecture Notes](https://faculty.washington.edu/otoomet/machineLearning.pdf), Ch 7.3.2 ''Smoothing: how to compute probabilities with too\n",
    "few data'', page 263.\n",
    "\n",
    "Laplace smoothing does not add anything here but it is is a crucial\n",
    "tool when we move to Naive Bayes later.\n",
    "\n",
    "15. (5pt) Add such smoothing to the model.  You can either literally add two such lines of data, or alternatively manipulate the way you compute the probabilities.\n",
    "\n",
    "\n",
    "16. (5pt) Repeat the tasks above: compute the probabilities, do predictions, compute the accuracy, precision, recall for all words.  \n",
    "\n",
    "\n",
    "17. (4pt) Comment on the results.  Does smoothing improve the overall performance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df335fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d69c20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
