{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706ff59b",
   "metadata": {},
   "source": [
    "# INFO 371 - Comparing Models\n",
    "\n",
    "For this homework assignment, we are going to ask to you to put together everything you have learned so far to analyze 4 very different datasets! The goal is for you to gain experience and intuition for how each of the supervised learning algorithms perform in different scenarios. \n",
    "\n",
    "# Datasets \n",
    "There are four datasets you will need to analyze (one of which you have worked with in past asignments!) \n",
    "\n",
    "## Dataset 1: Soybean Dataset\n",
    "With this dataset (``soybean.csv``) your goal is to predict which of the four diseases a particular soy-bean plant has. This datset was part of a project used to build a survey to diagnose crops (you can read more about this work [here](http://www.mli.gmu.edu/papers/79-80/80-2.pdf). The dataset has the following features: \n",
    "\n",
    "* Month: which month was the plant disease found, represented as a number meaning april is 4 ect.\n",
    "* Plant Stand: describes how much of the soybean plant is above the soil \n",
    "    * 0 is avg,  1 is below avg\n",
    "* Precipitation:  0: below avg, 1: avg, 2: above avg\n",
    "* temp:  0: below avg, 1: avg, 2: above avg\n",
    "* hail: 0 is no, 1 is yes \n",
    "* crop-hist\t\n",
    "    * 0: diff-lst-year\n",
    "    * 1: same-lst-yr\n",
    "    * 2: same-lst-two-yrs\n",
    "    * 3: same-lst-sev-yrs\n",
    "* area-damaged\n",
    "    * 0: scattered\n",
    "    * 1: low-areas\n",
    "    * 2: upper-areas\n",
    "    * 3:whole-field\n",
    "* severity\n",
    "    * 0 minor,1: potentially severe, 2: severe\n",
    "* seed-tmt: 0: None, 1: used fungicide \n",
    "* germination: How much of the plants sprouted\n",
    "    * 0: 90-100%, 1: 80-89%, 2: 80% or less\t\n",
    "* plant-growth: 0: normal, 1: abnormal \n",
    "* leaves: 0: normal, 1: abnormal \n",
    "* leafspots-halo: 0: absent: 1: has yellow halos, 2: has halos, but not yellow halos\n",
    "* leafspots-margin: 0: water-soaked margin,1: no margin, 2: does not apply \t\n",
    "* leafspot-size: 0: less than 1/8 in, 1: bigger than 1/8 in, 2: does not apply\n",
    "* leaf-shread: \t0: absent, 1: present\n",
    "* leaf-malf: 0: absent, 1: present\n",
    "* leaf-mild: 0: absent, 1: present on upper surface ,2: present on lower surface\n",
    "* stem: 0: normal, 1: not normal \n",
    "* lodging: 0: absent, 1: present\n",
    "* stem-cankers: 0: absent, 1: found below soil, 2: found above soil, 3: found above second node\n",
    "* canker-lesion\t0: absent,1: brown color ,2: dark brown to black color, 3: tan\n",
    "* fruiting-bodies: \t0: absent, 1: present\n",
    "* extern_decay: 0: absent, 1: firm and dry, 2: watery \n",
    "* mycelium: 0: absent, 1: present\t\n",
    "* int-discolor: 0: none, 1: black, 2: brown \n",
    "* sclerotia: 0: absent, 1: present\t\n",
    "* fruit-pods: 0: normal, 1: diseased, 2: few present, 3: does not apply \t\n",
    "* fruit-spots: 0: absent, 1: colored, 2: brown with black specks, 3: distorted, 4: does not apply\t\n",
    "* seed: 0: normal, 1: not normal\n",
    "* mold-groth: 0: absent, 1: present\n",
    "* seed-discolor: 0: absent, 1: present\n",
    "* seed-size: 0: average, 1: below average \t\n",
    "* shriveling: 0: absent, 1: present\t\n",
    "* roots\t: 0: normal, 1: diseased \n",
    "\n",
    "\n",
    "* Label: which of the diseases that plat has\n",
    "    * D1: diaporthe stem canker\n",
    "    * D2: charcoal rot\n",
    "    * D3: rhizoctonia root rot\n",
    "    * D4: phytophthora rot\n",
    "\n",
    "\n",
    "## Dataset 2: Iris Dataset\n",
    "With this dataset (``iris.csv``), your goal to categorize Iris flowers based on the four measurements (aka: features) into the correct species (aka: labels) setosa, virginica, and versicolor. The data contains 50 flowers of each species (150 in total), and four measurements for each species (petal length and width, and sepal length and width). All of these are numeric measures.\n",
    "\n",
    "## Dataset 3: Skin Segmentation Dataset\n",
    "With this dataset (``Skin_NonSkin.tsv``), your goal is to predict whether a particular pixel is a skin tone or not a skin tone. Each row in the dataset is a pixel taken from a random image. The features are the values in the B, G, R color space (i.e blue, green, red). \n",
    "\n",
    "## Dataset 4: Taiwanese Bankruptcy Prediction Dataset \n",
    "With this dataset (``bankruptcy.csv``), your goal is to predict whether a company went backrupt or not. This dataset was collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange. The dataset has the following features: \n",
    "\n",
    "* Bankrupt?: Class label\n",
    "* ROA(C) before interest and depreciation before interest: Return On Total Assets(C)\n",
    "* ROA(A) before interest and % after tax: Return On Total Assets(A)\n",
    "* ROA(B) before interest and depreciation after tax: Return On Total Assets(B)\n",
    "* Operating Gross Margin: Gross Profit/Net Sales\n",
    "* Realized Sales Gross Margin: Realized Gross Profit/Net Sales\n",
    "* Operating Profit Rate: Operating Income/Net Sales\n",
    "* Pre-tax net Interest Rate: Pre-Tax Income/Net Sales\n",
    "* After-tax net Interest Rate: Net Income/Net Sales\n",
    "* Non-industry income and expenditure/revenue: Net Non-operating Income Ratio\n",
    "* Continuous interest rate (after tax): Net Income-Exclude Disposal Gain or Loss/Net Sales\n",
    "* Operating Expense Rate: Operating Expenses/Net Sales\n",
    "* Research and development expense rate: (Research and Development Expenses)/Net Sales\n",
    "* Cash flow rate: Cash Flow from Operating/Current Liabilities\n",
    "* Interest-bearing debt interest rate: Interest-bearing Debt/Equity\n",
    "* Tax rate (A): Effective Tax Rate\n",
    "* Net Value Per Share (B): Book Value Per Share(B)\n",
    "* Net Value Per Share (A): Book Value Per Share(A)\n",
    "* Net Value Per Share (C): Book Value Per Share(C)\n",
    "* Persistent EPS in the Last Four Seasons: EPS-Net Income\n",
    "* Cash Flow Per Share\n",
    "* Revenue Per Share (Yuan ¥): Sales Per Share\n",
    "* Operating Profit Per Share (Yuan ¥): Operating Income Per Share\n",
    "* Per Share Net profit before tax (Yuan ¥): Pretax Income Per Share\n",
    "* Realized Sales Gross Profit Growth Rate\n",
    "* Operating Profit Growth Rate: Operating Income Growth\n",
    "* After-tax Net Profit Growth Rate: Net Income Growth\n",
    "* Regular Net Profit Growth Rate: Continuing Operating Income after Tax Growth\n",
    "* Continuous Net Profit Growth Rate: Net Income-Excluding Disposal Gain or Loss Growth\n",
    "* Total Asset Growth Rate: Total Asset Growth\n",
    "* Net Value Growth Rate: Total Equity Growth\n",
    "* Total Asset Return Growth Rate Ratio: Return on Total Asset Growth\n",
    "* Cash Reinvestment %: Cash Reinvestment Ratio\n",
    "* Current Ratio\n",
    "* Quick Ratio: Acid Test\n",
    "* Interest Expense Ratio: Interest Expenses/Total Revenue\n",
    "* Total debt/Total net worth: Total Liability/Equity Ratio\n",
    "* Debt ratio %: Liability/Total Assets\n",
    "* Net worth/Assets: Equity/Total Assets\n",
    "* Long-term fund suitability ratio (A): (Long-term Liability+Equity)/Fixed Assets\n",
    "* Borrowing dependency: Cost of Interest-bearing Debt\n",
    "* Contingent liabilities/Net worth: Contingent Liability/Equity\n",
    "* Operating profit/Paid-in capital: Operating Income/Capital\n",
    "* Net profit before tax/Paid-in capital: Pretax Income/Capital\n",
    "* Inventory and accounts receivable/Net value: (Inventory+Accounts Receivables)/Equity\n",
    "* Total Asset Turnover\n",
    "* Accounts Receivable Turnover\n",
    "* Average Collection Days: Days Receivable Outstanding\n",
    "* Inventory Turnover Rate (times)\n",
    "* Fixed Assets Turnover Frequency\n",
    "* Net Worth Turnover Rate (times): Equity Turnover\n",
    "* Revenue per person: Sales Per Employee\n",
    "* Operating profit per person: Operation Income Per Employee\n",
    "* Allocation rate per person: Fixed Assets Per Employee\n",
    "* Working Capital to Total Assets\n",
    "* Quick Assets/Total Assets\n",
    "* Current Assets/Total Assets\n",
    "* Cash/Total Assets\n",
    "* Quick Assets/Current Liability\n",
    "* Cash/Current Liability\n",
    "* Current Liability to Assets\n",
    "* Operating Funds to Liability\n",
    "* Inventory/Working Capital\n",
    "* Inventory/Current Liability\n",
    "* Current Liabilities/Liability\n",
    "* Working Capital/Equity\n",
    "* Current Liabilities/Equity\n",
    "* Long-term Liability to Current Assets\n",
    "* Retained Earnings to Total Assets\n",
    "* Total income/Total expense\n",
    "* Total expense/Assets\n",
    "* Current Asset Turnover Rate: Current Assets to Sales\n",
    "* Quick Asset Turnover Rate: Quick Assets to Sales\n",
    "* Working capitcal Turnover Rate: Working Capital to Sales\n",
    "* Cash Turnover Rate: Cash to Sales\n",
    "* Cash Flow to Sales\n",
    "* Fixed Assets to Assets\n",
    "* Current Liability to Liability\n",
    "* Current Liability to Equity\n",
    "* Equity to Long-term Liability\n",
    "* Cash Flow to Total Assets\n",
    "* Cash Flow to Liability\n",
    "* CFO to Assets\n",
    "* Cash Flow to Equity\n",
    "* Current Liability to Current Assets\n",
    "* Liability-Assets Flag: 1 if Total Liability exceeds Total Assets, 0 otherwise\n",
    "* Net Income to Total Assets\n",
    "* Total assets to GNP price\n",
    "* No-credit Interval\n",
    "* Gross Profit to Sales\n",
    "* Net Income to Stockholder's Equity\n",
    "* Liability to Equity\n",
    "* Degree of Financial Leverage (DFL)\n",
    "* Interest Coverage Ratio (Interest expense to EBIT)\n",
    "* Net Income Flag: 1 if Net Income is Negative for the last two years, 0 otherwise\n",
    "* Equity to Liability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b651d67",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset Exploration \n",
    "\n",
    "1. Load in each dataset, then report on how many rows and columns each dataset has AND the label distibution (i.e how many data items are in each label -- for example, the iris dataset has 50 flowers across each of the 3 flower species labels)\n",
    "\n",
    "\n",
    "2. Based on this dataset size information and the given dataset decriptions alone, which of the supervised learning models (Logistic Regression, K-NN, Naive Bayes, SVMs, decision trees, or random forests) do you think will peform best on each dataset? Which do you think will perform worst? Explain your reasoning for each choice. \n",
    "    * NOTE: DO NOT RUN ANY EXPERIMENTS BEFOREHAND! You should use your intuition and class discussions for this! \n",
    "    \n",
    "\n",
    "3. Get a baseline accuracy using the naive/constant model (i.e. a model where you assign the same label to all your testing data and that label is the one that appeared the most in your training data) for each of the datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fbb8b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:08:36.807638Z",
     "start_time": "2024-05-17T23:08:36.670359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soybean dataset rows, columns:  (47, 36)\n",
      "iris dataset rows, columns:  (150, 5)\n",
      "skin dataset rows, columns:  (245057, 4)\n",
      "bank dataset rows, columns:  (6819, 96)\n",
      "\n",
      "soybean dataset label quantities:  Label\n",
      "D4    17\n",
      "D1    10\n",
      "D2    10\n",
      "D3    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "iris dataset label quantities:  Species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "skin dataset label quantities:  Label\n",
      "2    194198\n",
      "1     50859\n",
      "Name: count, dtype: int64\n",
      "\n",
      "bank dataset label quantities:  Bankrupt?\n",
      "0            6599\n",
      "1             220\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#code goes here\n",
    "soy = pd.read_csv('soybean.csv')\n",
    "iris = pd.read_csv('iris.csv', delimiter=\"\\t\")\n",
    "skin = pd.read_csv('Skin_NonSkin.tsv', delimiter='\\t')\n",
    "bank = pd.read_csv('bankruptcy.csv')\n",
    "\n",
    "print(\"soybean dataset rows, columns: \", soy.shape)\n",
    "print(\"iris dataset rows, columns: \", iris.shape)\n",
    "print(\"skin dataset rows, columns: \", skin.shape)\n",
    "print(\"bank dataset rows, columns: \", bank.shape)\n",
    "print()\n",
    "print(\"soybean dataset label quantities: \", soy.Label.value_counts())\n",
    "print()\n",
    "print(\"iris dataset label quantities: \", iris.Species.value_counts())\n",
    "print()\n",
    "print(\"skin dataset label quantities: \", skin.Label.value_counts())\n",
    "print()\n",
    "print(\"bank dataset label quantities: \", bank[[\"Bankrupt?\"]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b4c2064ef647",
   "metadata": {},
   "source": [
    "Based on the results of the analysis I believe the skin dataset will perform best with KNN as it is quite large. I believe the iris dataset will work best with SVM given its size and simplicity. I believe the soybean dataset may work well with a tree structure given its smaller size, perhaps logistic regression too. Lastly I believe the banking dataset will work best with naive bayes given its width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbc367af4cde14a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:08:36.855203Z",
     "start_time": "2024-05-17T23:08:36.808582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive soy model accuracy:  0.3\n",
      "Naive iris model accuracy:  0.0\n",
      "Naive skin model accuracy:  0.7939076144617645\n",
      "Naive bank model accuracy:  0.9604105571847508\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "#NAIVE MODELS\n",
    "from sklearn.model_selection import train_test_split\n",
    "x1 = soy.drop(columns='Label')\n",
    "y1 = soy['Label']\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.2)\n",
    "\n",
    "x2 = iris.drop(columns='Species')\n",
    "y2 = iris['Species']\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.2)\n",
    "\n",
    "x3 = skin.drop(columns='Label')\n",
    "y3 = skin['Label']\n",
    "x3_train, x3_test, y3_train, y3_test = train_test_split(x3, y3, test_size=0.2)\n",
    "\n",
    "x4 = bank.drop(columns='Bankrupt?')\n",
    "y4 = bank['Bankrupt?']\n",
    "x4_train, x4_test, y4_train, y4_test = train_test_split(x4, y4, test_size=0.2)\n",
    "\n",
    "naive_soy = np.full((len(y1_test)), \"D4\")\n",
    "naive_iris = np.full((len(y2_test)), \"Setosa\")\n",
    "naive_skin = np.full((len(y3_test)), 2)\n",
    "naive_bank = np.full((len(y4_test)), 0)\n",
    "\n",
    "print(\"Naive soy model accuracy: \",accuracy_score(y1_test, naive_soy))\n",
    "print(\"Naive iris model accuracy: \",accuracy_score(y2_test, naive_iris))\n",
    "print(\"Naive skin model accuracy: \",accuracy_score(y3_test, naive_skin))\n",
    "print(\"Naive bank model accuracy: \",accuracy_score(y4_test, naive_bank))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d859e76",
   "metadata": {},
   "source": [
    "# Model Comparisons and Analysis\n",
    "\n",
    "4.Compare the average testing accuracy as well as runtime across each of the 4 datasets for each of the following experiment set-ups: \n",
    "\n",
    "    1. Logistic Regression \n",
    "\n",
    "    2. K-Nearest Neighbors (K-NN) for euclidean and manhattan distance and for k values = {5, 10, 15, 20, 25, 30, 35, 40, 45, 50}\n",
    "\n",
    "    3. Naive Bayes \n",
    "\n",
    "    4. Decision Trees using citerion = \"entropy\" across various values for max_depth  \n",
    "    \n",
    "    5. Random Forests using citerion = \"entropy\" across various values for num_estimators (try to pick atleast 3 numbers: one using the \"ideal\" value we talked about in class, one less than the ideal, and one much greater than the ideal) \n",
    "    \n",
    "    6. SVMs using both the Linear and RBF kernel\n",
    "    \n",
    "\n",
    "Note: you may use k-fold cross validation for these experiments\n",
    "\n",
    "\n",
    "5. Were your predicitons from step 2 correct? Why or why not? \n",
    "\n",
    "\n",
    "6. Which model would you chose for each dataset? Explain your reasoning and use the results of your experiments and any domain knowledge you have to support your argument. Make sure you discuss in detail the trade-offs you considered as part of this analysis. Your response here should be pretty detailed. \n",
    "\n",
    "\n",
    "7. Why do you think certain models performed better on some of your datasets versus others? Are there certain attributes/aspects of each dataset that some machine learning models prefer? Explain your reasoning here in detail. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9388ec54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:08:37.249768Z",
     "start_time": "2024-05-17T23:08:36.856138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic soy performance:  1.0\n",
      "logistic soy speed:  0.015298843383789062\n",
      "\n",
      "logistic iris performance:  1.0\n",
      "logistic iris speed:  0.01629781723022461\n",
      "\n",
      "logistic skin performance:  0.9200195870399086\n",
      "logistic skin speed:  1.3564426898956299\n",
      "\n",
      "logistic bank performance:  0.9530791788856305\n",
      "logistic bank speed:  1.699185848236084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code goes here \n",
    "\n",
    "#Logistic Regression\n",
    "t0 = time.time()\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(x1_train, y1_train)\n",
    "print('logistic soy performance: ', model1.score(x1_test, y1_test))\n",
    "t1 = time.time()\n",
    "total_time1 = t1 - t0\n",
    "print('logistic soy speed: ',total_time1)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model2 = LogisticRegression(max_iter=1000)\n",
    "model2.fit(x2_train, y2_train)\n",
    "print('logistic iris performance: ', model2.score(x2_test, y2_test))\n",
    "t1 = time.time()\n",
    "total_time2 = t1 - t0\n",
    "print('logistic iris speed: ',total_time2)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model3 = LogisticRegression()\n",
    "model3.fit(x3_train, y3_train)\n",
    "print('logistic skin performance: ', model3.score(x3_test, y3_test))\n",
    "t1 = time.time()\n",
    "total_time3 = t1 - t0\n",
    "print('logistic skin speed: ',total_time3)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model4 = LogisticRegression(max_iter=1000)\n",
    "model4.fit(x4_train, y4_train)\n",
    "print('logistic bank performance: ', model4.score(x4_test, y4_test))\n",
    "t1 = time.time()\n",
    "total_time4 = t1 - t0\n",
    "print('logistic bank speed: ',total_time4)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ca7d3d69a7bf3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:08:47.391266Z",
     "start_time": "2024-05-17T23:08:37.250772Z"
    }
   },
   "outputs": [],
   "source": [
    "#KNN EUCLIDEAN\n",
    "#model1_accuracy = []\n",
    "model2_accuracy = []\n",
    "model3_accuracy = []\n",
    "model4_accuracy = []\n",
    "model2_time = []\n",
    "model3_time = []\n",
    "model4_time = []\n",
    "for k in range(5,51, 5):\n",
    "        \n",
    "###NOTE THE SOY DATASET IS TOO SMALL TO USE FOR KNN GIVEN K > 40 does not work with training set size of 37.\n",
    "        \n",
    "        #model1 = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "        #model1.fit(x1_train, y1_train)\n",
    "        #model1_accuracy.append(model1.score(x1_test, y1_test))\n",
    "        \n",
    "        t0 = time.time()\n",
    "        model2 = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "        model2.fit(x2_train, y2_train)\n",
    "        model2_accuracy.append(model2.score(x2_test, y2_test))\n",
    "        t1 = time.time()\n",
    "        total_time2 = t1 - t0\n",
    "        model2_time.append(total_time2)\n",
    "        \n",
    "            \n",
    "        t0 = time.time()\n",
    "        model3 = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "        model3.fit(x3_train, y3_train)\n",
    "        model3_accuracy.append(model3.score(x3_test, y3_test))\n",
    "        t1 = time.time()\n",
    "        total_time3 = t1 - t0\n",
    "        model3_time.append(total_time3)\n",
    "        \n",
    "           \n",
    "        t0 = time.time()\n",
    "        model4 = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "        model4.fit(x4_train, y4_train)\n",
    "        model4_accuracy.append(model4.score(x4_test, y4_test))\n",
    "        t1 = time.time()\n",
    "        total_time4 = t1 - t0\n",
    "        model4_time.append(total_time4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad48e008d090e985",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:08:47.406780Z",
     "start_time": "2024-05-17T23:08:47.392264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN EUCLIDEAN MODELLING\n",
      "Model iris mean Accuracies: 0.95\n",
      "Model skin mean Accuracies: 0.9989104709050844\n",
      "Model bank mean Accuracies: 0.9599706744868037\n",
      "Model iris mean Times: 0.006351351737976074\n",
      "Model skin mean Times: 2.9919050216674803\n",
      "Model bank mean Times: 0.11887910366058349\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean\n",
    "print(\"KNN EUCLIDEAN MODELLING\")\n",
    "print(\"Model iris mean Accuracies:\", mean(model2_accuracy))\n",
    "print(\"Model skin mean Accuracies:\", mean(model3_accuracy))\n",
    "print(\"Model bank mean Accuracies:\", mean(model4_accuracy))\n",
    "print(\"Model iris mean Times:\", mean(model2_time))\n",
    "print(\"Model skin mean Times:\", mean(model3_time))\n",
    "print(\"Model bank mean Times:\", mean(model4_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c363ba4104750c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:08:57.545304Z",
     "start_time": "2024-05-17T23:08:47.408290Z"
    }
   },
   "outputs": [],
   "source": [
    "#KNN manhattan\n",
    "#model1_accuracy = []\n",
    "model2_accuracy = []\n",
    "model3_accuracy = []\n",
    "model4_accuracy = []\n",
    "model2_time = []\n",
    "model3_time = []\n",
    "model4_time = []\n",
    "for k in range(5,51, 5):\n",
    "        \n",
    "###NOTE THE SOY DATASET IS TOO SMALL TO USE FOR KNN GIVEN K > 40 does not work with training set size of 37.\n",
    "        \n",
    "        #model1 = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "        #model1.fit(x1_train, y1_train)\n",
    "        #model1_accuracy.append(model1.score(x1_test, y1_test))\n",
    "        \n",
    "        t0 = time.time()\n",
    "        model2 = KNeighborsClassifier(n_neighbors=k, metric='manhattan')\n",
    "        model2.fit(x2_train, y2_train)\n",
    "        model2_accuracy.append(model2.score(x2_test, y2_test))\n",
    "        t1 = time.time()\n",
    "        total_time2 = t1 - t0\n",
    "        model2_time.append(total_time2)\n",
    "        \n",
    "            \n",
    "        t0 = time.time()\n",
    "        model3 = KNeighborsClassifier(n_neighbors=k, metric='manhattan')\n",
    "        model3.fit(x3_train, y3_train)\n",
    "        model3_accuracy.append(model3.score(x3_test, y3_test))\n",
    "        t1 = time.time()\n",
    "        total_time3 = t1 - t0\n",
    "        model3_time.append(total_time3)\n",
    "        \n",
    "           \n",
    "        t0 = time.time()\n",
    "        model4 = KNeighborsClassifier(n_neighbors=k, metric='manhattan')\n",
    "        model4.fit(x4_train, y4_train)\n",
    "        model4_accuracy.append(model4.score(x4_test, y4_test))\n",
    "        t1 = time.time()\n",
    "        total_time4 = t1 - t0\n",
    "        model4_time.append(total_time4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e384b07cb67290e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:08:57.560829Z",
     "start_time": "2024-05-17T23:08:57.546305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN MANHATTAN MODELLING\n",
      "Model iris mean Accuracies: 0.96\n",
      "Model skin mean Accuracies: 0.9989002693217988\n",
      "Model bank mean Accuracies: 0.960117302052786\n",
      "Model iris mean Times: 0.006132054328918457\n",
      "Model skin mean Times: 2.9271103620529173\n",
      "Model bank mean Times: 0.238899564743042\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN MANHATTAN MODELLING\")\n",
    "print(\"Model iris mean Accuracies:\", mean(model2_accuracy))\n",
    "print(\"Model skin mean Accuracies:\", mean(model3_accuracy))\n",
    "print(\"Model bank mean Accuracies:\", mean(model4_accuracy))\n",
    "print(\"Model iris mean Times:\", mean(model2_time))\n",
    "print(\"Model skin mean Times:\", mean(model3_time))\n",
    "print(\"Model bank mean Times:\", mean(model4_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d1ec90f74a6b2b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:08:57.639501Z",
     "start_time": "2024-05-17T23:08:57.561829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes soy performance:  1.0\n",
      "Naive bayes soy speed:  0.009855031967163086\n",
      "\n",
      "Naive bayes iris performance:  0.9333333333333333\n",
      "Naive bayes iris speed:  0.004797697067260742\n",
      "\n",
      "Naive bayes skin performance:  0.9447074185913654\n",
      "Naive bayes skin speed:  0.0345311164855957\n",
      "\n",
      "NOTE: NB classification does not work due to the presence of negative values in the BANK dataset\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "#Naive bayes\n",
    "\n",
    "t0 = time.time()\n",
    "model1 = CategoricalNB()\n",
    "model1.fit(x1_train, y1_train)\n",
    "print('Naive bayes soy performance: ', model1.score(x1_test, y1_test))\n",
    "t1 = time.time()\n",
    "total_time1 = t1 - t0\n",
    "print('Naive bayes soy speed: ',total_time1)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model2 = CategoricalNB()\n",
    "model2.fit(x2_train, y2_train)\n",
    "print('Naive bayes iris performance: ', model2.score(x2_test, y2_test))\n",
    "t1 = time.time()\n",
    "total_time2 = t1 - t0\n",
    "print('Naive bayes iris speed: ',total_time2)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model3 = CategoricalNB()\n",
    "model3.fit(x3_train, y3_train)\n",
    "print('Naive bayes skin performance: ', model3.score(x3_test, y3_test))\n",
    "t1 = time.time()\n",
    "total_time3 = t1 - t0\n",
    "print('Naive bayes skin speed: ',total_time3)\n",
    "print()\n",
    "#t0 = time.time()\n",
    "#model4 = CategoricalNB()\n",
    "#model4.fit(x4_train, y4_train)\n",
    "#print('Naive bayes bank performance: ', model4.score(x4_test, y4_test))\n",
    "#t1 = time.time()\n",
    "#total_time4 = t1 - t0\n",
    "#print('Naive bayes bank speed: ',total_time4)\n",
    "print(\"NOTE: NB classification does not work due to the presence of negative values in the BANK dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a60f1adaddd512",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:09:06.052766Z",
     "start_time": "2024-05-17T23:08:57.640502Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#decision trees with varying depth\n",
    "\n",
    "model1_accuracy = []\n",
    "model2_accuracy = []\n",
    "model3_accuracy = []\n",
    "model4_accuracy = []\n",
    "model1_time = []\n",
    "model2_time = []\n",
    "model3_time = []\n",
    "model4_time = []\n",
    "for depth in range(1,30):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        d_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = depth)\n",
    "        m = d_tree.fit(x1_train, y1_train)\n",
    "        model1_accuracy.append(m.score(x1_test, y1_test))\n",
    "        t1 = time.time()\n",
    "        total_time1 = t1 - t0\n",
    "        model1_time.append(total_time1)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        d_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = depth)\n",
    "        m = d_tree.fit(x2_train, y2_train)\n",
    "        model2_accuracy.append(m.score(x2_test, y2_test))\n",
    "        t1 = time.time()\n",
    "        total_time2 = t1 - t0\n",
    "        model2_time.append(total_time2)\n",
    "            \n",
    "        t0 = time.time()\n",
    "        d_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = depth)\n",
    "        m = d_tree.fit(x3_train, y3_train)\n",
    "        model3_accuracy.append(m.score(x3_test, y3_test))\n",
    "        t1 = time.time()\n",
    "        total_time3 = t1 - t0\n",
    "        model3_time.append(total_time3)\n",
    "        \n",
    "           \n",
    "        t0 = time.time()\n",
    "        d_tree = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = depth)\n",
    "        m = d_tree.fit(x4_train, y4_train)\n",
    "        model4_accuracy.append(m.score(x4_test, y4_test))\n",
    "        t1 = time.time()\n",
    "        total_time4 = t1 - t0\n",
    "        model4_time.append(total_time4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280a88b15f58ef64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:09:06.068797Z",
     "start_time": "2024-05-17T23:09:06.054767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TREE CLASSIFIER MODELLING\n",
      "Model soy mean Accuracies: 0.9758620689655173\n",
      "Model iris mean Accuracies: 0.9390804597701149\n",
      "Model skin mean Accuracies: 0.9886804638976522\n",
      "Model bank mean Accuracies: 0.9471129537870364\n",
      "Model soy mean Times: 0.0031600409540636786\n",
      "Model iris mean Times: 0.0027048587799072266\n",
      "Model skin mean Times: 0.16841128776813374\n",
      "Model bank mean Times: 0.3577179744325835\n"
     ]
    }
   ],
   "source": [
    "print(\"TREE CLASSIFIER MODELLING\")\n",
    "print(\"Model soy mean Accuracies:\", mean(model1_accuracy))\n",
    "print(\"Model iris mean Accuracies:\", mean(model2_accuracy))\n",
    "print(\"Model skin mean Accuracies:\", mean(model3_accuracy))\n",
    "print(\"Model bank mean Accuracies:\", mean(model4_accuracy))\n",
    "print(\"Model soy mean Times:\", mean(model1_time))\n",
    "print(\"Model iris mean Times:\", mean(model2_time))\n",
    "print(\"Model skin mean Times:\", mean(model3_time))\n",
    "print(\"Model bank mean Times:\", mean(model4_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1df0322fc86bbce0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:09:39.625426Z",
     "start_time": "2024-05-17T23:09:06.069798Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble as e\n",
    "\n",
    "#random forests with varying depth\n",
    "\n",
    "model1_accuracy = []\n",
    "model2_accuracy = []\n",
    "model3_accuracy = []\n",
    "model4_accuracy = []\n",
    "model1_time = []\n",
    "model2_time = []\n",
    "model3_time = []\n",
    "model4_time = []\n",
    "for trees in range(1,101, 10):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        forest = e.RandomForestClassifier(criterion = 'entropy', n_estimators = trees)\n",
    "        m = forest.fit(x1_train, y1_train)\n",
    "        model1_accuracy.append(m.score(x1_test, y1_test))\n",
    "        t1 = time.time()\n",
    "        total_time1 = t1 - t0\n",
    "        model1_time.append(total_time1)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        forest = e.RandomForestClassifier(criterion = 'entropy', n_estimators = trees)\n",
    "        m = forest.fit(x2_train, y2_train)\n",
    "        model2_accuracy.append(m.score(x2_test, y2_test))\n",
    "        t1 = time.time()\n",
    "        total_time2 = t1 - t0\n",
    "        model2_time.append(total_time2)\n",
    "            \n",
    "        t0 = time.time()\n",
    "        forest = e.RandomForestClassifier(criterion = 'entropy', n_estimators = trees)\n",
    "        m = forest.fit(x3_train, y3_train)\n",
    "        model3_accuracy.append(m.score(x3_test, y3_test))\n",
    "        t1 = time.time()\n",
    "        total_time3 = t1 - t0\n",
    "        model3_time.append(total_time3)\n",
    "        \n",
    "           \n",
    "        t0 = time.time()\n",
    "        forest = e.RandomForestClassifier(criterion = 'entropy', n_estimators = trees)\n",
    "        m = forest.fit(x4_train, y4_train)\n",
    "        model4_accuracy.append(m.score(x4_test, y4_test))\n",
    "        t1 = time.time()\n",
    "        total_time4 = t1 - t0\n",
    "        model4_time.append(total_time4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1381310af9a0538f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:09:39.641486Z",
     "start_time": "2024-05-17T23:09:39.626428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST CLASSIFIER MODELLING\n",
      "Model soy mean Accuracies: 1.0\n",
      "Model iris mean Accuracies: 0.9333333333333333\n",
      "Model skin mean Accuracies: 0.9995103240022851\n",
      "Model bank mean Accuracies: 0.968475073313783\n",
      "Model soy mean Times: 0.04136018753051758\n",
      "Model iris mean Times: 0.042569947242736814\n",
      "Model skin mean Times: 2.361792731285095\n",
      "Model bank mean Times: 0.9064349174499512\n"
     ]
    }
   ],
   "source": [
    "print(\"RANDOM FOREST CLASSIFIER MODELLING\")\n",
    "print(\"Model soy mean Accuracies:\", mean(model1_accuracy))\n",
    "print(\"Model iris mean Accuracies:\", mean(model2_accuracy))\n",
    "print(\"Model skin mean Accuracies:\", mean(model3_accuracy))\n",
    "print(\"Model bank mean Accuracies:\", mean(model4_accuracy))\n",
    "print(\"Model soy mean Times:\", mean(model1_time))\n",
    "print(\"Model iris mean Times:\", mean(model2_time))\n",
    "print(\"Model skin mean Times:\", mean(model3_time))\n",
    "print(\"Model bank mean Times:\", mean(model4_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96adc7457bc09ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:09:50.546311Z",
     "start_time": "2024-05-17T23:09:39.642488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR SVM soy performance:  1.0\n",
      "LINEAR SVM soy speed:  0.004519224166870117\n",
      "\n",
      "LINEAR SVM iris performance:  0.8666666666666667\n",
      "LINEAR SVM iris speed:  0.004996776580810547\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Ramstad\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Henry Ramstad\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR SVM skin performance:  0.8698890067738513\n",
      "LINEAR SVM skin speed:  10.197792768478394\n",
      "\n",
      "LINEAR SVM bank performance:  0.968475073313783\n",
      "LINEAR SVM bank speed:  0.6820039749145508\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Henry Ramstad\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#SVM LINEAR\n",
    "from sklearn import svm \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "t0 = time.time()\n",
    "model1 = LinearSVC()\n",
    "model1.fit(x1_train, y1_train)\n",
    "print('LINEAR SVM soy performance: ', model1.score(x1_test, y1_test))\n",
    "t1 = time.time()\n",
    "total_time1 = t1 - t0\n",
    "print('LINEAR SVM soy speed: ',total_time1)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model2 =  LinearSVC()\n",
    "model2.fit(x2_train, y2_train)\n",
    "print('LINEAR SVM iris performance: ', model2.score(x2_test, y2_test))\n",
    "t1 = time.time()\n",
    "total_time2 = t1 - t0\n",
    "print('LINEAR SVM iris speed: ',total_time2)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model3 =  LinearSVC(max_iter = 1000)\n",
    "model3.fit(x3_train, y3_train)\n",
    "print('LINEAR SVM skin performance: ', model3.score(x3_test, y3_test))\n",
    "t1 = time.time()\n",
    "total_time3 = t1 - t0\n",
    "print('LINEAR SVM skin speed: ',total_time3)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model4 =  LinearSVC(max_iter = 1000)\n",
    "model4.fit(x4_train, y4_train)\n",
    "print('LINEAR SVM bank performance: ', model4.score(x4_test, y4_test))\n",
    "t1 = time.time()\n",
    "total_time4 = t1 - t0\n",
    "print('LINEAR SVM bank speed: ',total_time4)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1ab9fb41939c37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T23:11:43.993079Z",
     "start_time": "2024-05-17T23:11:24.711489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM soy performance:  0.6\n",
      "SVM soy speed:  0.0035092830657958984\n",
      "\n",
      "SVM iris performance:  0.9\n",
      "SVM iris speed:  0.0020093917846679688\n",
      "\n",
      "SVM skin performance:  0.9984493593405697\n",
      "SVM skin speed:  18.957127809524536\n",
      "\n",
      "SVM bank performance:  0.9706744868035191\n",
      "SVM bank speed:  0.3119387626647949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM RBF\n",
    " \n",
    "\n",
    "t0 = time.time()\n",
    "model1 = svm.SVC()\n",
    "model1.fit(x1_train, y1_train)\n",
    "print('SVM soy performance: ', model1.score(x1_test, y1_test))\n",
    "t1 = time.time()\n",
    "total_time1 = t1 - t0\n",
    "print('SVM soy speed: ',total_time1)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model2 =  svm.SVC()\n",
    "model2.fit(x2_train, y2_train)\n",
    "print('SVM iris performance: ', model2.score(x2_test, y2_test))\n",
    "t1 = time.time()\n",
    "total_time2 = t1 - t0\n",
    "print('SVM iris speed: ',total_time2)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model3 =  svm.SVC()\n",
    "model3.fit(x3_train, y3_train)\n",
    "print('SVM skin performance: ', model3.score(x3_test, y3_test))\n",
    "t1 = time.time()\n",
    "total_time3 = t1 - t0\n",
    "print('SVM skin speed: ',total_time3)\n",
    "print()\n",
    "t0 = time.time()\n",
    "model4 =  svm.SVC()\n",
    "model4.fit(x4_train, y4_train)\n",
    "print('SVM bank performance: ', model4.score(x4_test, y4_test))\n",
    "t1 = time.time()\n",
    "total_time4 = t1 - t0\n",
    "print('SVM bank speed: ',total_time4)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde892d06b7ddb4",
   "metadata": {},
   "source": [
    "ANALYSIS:\n",
    "\n",
    "for purposes of viewing I will copy and paste the major results of the analysis to compare\n",
    "\n",
    "NAIVE\\\n",
    "Naive soy model accuracy:  0.4\\\n",
    "Naive iris model accuracy:  0.0\\\n",
    "Naive skin model accuracy:  0.791581653472619\\\n",
    "Naive bank model accuracy:  0.9736070381231672"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d2ddc04fbfd17",
   "metadata": {},
   "source": [
    "LOGISTIC\\\n",
    "logistic soy performance:  1.0\\\n",
    "logistic soy speed:  0.010515928268432617\n",
    "\n",
    "logistic iris performance:  0.9666666666666667\\\n",
    "logistic iris speed:  0.012507200241088867\n",
    "\n",
    "logistic skin performance:  0.9194687015424794\\\n",
    "logistic skin speed:  0.2614462375640869\n",
    "\n",
    "logistic bank performance:  0.968475073313783\\\n",
    "logistic bank speed:  0.23293566703796387"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684187659c7d4c0",
   "metadata": {},
   "source": [
    "KNN EUCLIDEAN MODELLING\\\n",
    "KNN NOT POSSIBLE FOR SOY DATASET*\n",
    "\n",
    "Model iris mean Accuracies: 0.9933333333333334\\\n",
    "Model iris mean Times: 0.004015636444091797\n",
    "\n",
    "Model skin mean Accuracies: 0.9992695666367422\\\n",
    "Model skin mean Times: 0.9586779356002808\n",
    "\n",
    "Model bank mean Accuracies: 0.9730938416422287\\\n",
    "Model bank mean Times: 0.06626036167144775"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154ef5f67463592",
   "metadata": {},
   "source": [
    "KNN MANHATTAN MODELLING\\\n",
    "KNN NOT POSSIBLE FOR SOY DATASET*\n",
    "\n",
    "Model iris mean Accuracies: 0.9800000000000001\\\n",
    "Model iris mean Times: 0.0035050153732299806\n",
    "\n",
    "Model skin mean Accuracies: 0.999212437770342\\\n",
    "Model skin mean Times: 0.9401741981506347\n",
    "\n",
    "Model bank mean Accuracies: 0.973607038123167\\\n",
    "Model bank mean Times: 0.09838576316833496"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b16d56cd4dec48",
   "metadata": {},
   "source": [
    "NAIVE BAYES\\\n",
    "Naive bayes soy performance:  1.0\\\n",
    "Naive bayes soy speed:  0.006998777389526367\n",
    "\n",
    "Naive bayes iris performance:  0.9666666666666667\\\n",
    "Naive bayes iris speed:  0.0030012130737304688\n",
    "\n",
    "Naive bayes skin performance:  0.9470537827470823\\\n",
    "Naive bayes skin speed:  0.03899717330932617\n",
    "\n",
    "NOTE: NB classification does not work due to the presence of negative values in the BANK dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4861ad2cf7a19e8",
   "metadata": {},
   "source": [
    "TREE CLASSIFIER MODELLING\\\n",
    "Model soy mean Accuracies: 0.9827586206896551\\\n",
    "Model soy mean Times: 0.0020212305003199085\n",
    "\n",
    "Model iris mean Accuracies: 0.954022988505747\\\n",
    "Model iris mean Times: 0.001750946044921875\n",
    "\n",
    "Model skin mean Accuracies: 0.98875785521913\\\n",
    "Model skin mean Times: 0.1035644350380733\n",
    "\n",
    "Model bank mean Accuracies: 0.9602083122661543\\\n",
    "Model bank mean Times: 0.1991223220167489"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0327682716ccf6",
   "metadata": {},
   "source": [
    "RANDOM FOREST CLASSIFIER MODELLING\\\n",
    "Model soy mean Accuracies: 1.0\\\n",
    "Model soy mean Times: 0.0415095329284668\n",
    "\n",
    "Model iris mean Accuracies: 0.9533333333333334\\\n",
    "Model iris mean Times: 0.04161231517791748\n",
    "\n",
    "Model skin mean Accuracies: 0.9995184852689137\\\n",
    "Model skin mean Times: 2.3844290018081664\n",
    "\n",
    "Model bank mean Accuracies: 0.9733137829912024\\\n",
    "Model bank mean Times: 0.942660140991211\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6976ae992edfa001",
   "metadata": {},
   "source": [
    "LINEAR SVM\\\n",
    "LINEAR SVM soy performance:  1.0\\\n",
    "LINEAR SVM soy speed:  0.004519224166870117\n",
    "\n",
    "LINEAR SVM iris performance:  0.8666666666666667\\\n",
    "LINEAR SVM iris speed:  0.004996776580810547\n",
    "\n",
    "LINEAR SVM skin performance:  0.8698890067738513\\\n",
    "LINEAR SVM skin speed:  10.197792768478394\n",
    "\n",
    "LINEAR SVM bank performance:  0.968475073313783\\\n",
    "LINEAR SVM bank speed:  0.6820039749145508\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5aac063fd6c881",
   "metadata": {},
   "source": [
    "RBF SVM\\\n",
    "SVM soy performance:  0.6\\\n",
    "SVM soy speed:  0.0035092830657958984\n",
    "\n",
    "SVM iris performance:  0.9\\\n",
    "SVM iris speed:  0.0020093917846679688\n",
    "\n",
    "SVM skin performance:  0.9984493593405697\\\n",
    "SVM skin speed:  18.957127809524536\n",
    "\n",
    "SVM bank performance:  0.9706744868035191\\\n",
    "SVM bank speed:  0.3119387626647949\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807669735cd1ef6",
   "metadata": {},
   "source": [
    "NOTE: The data above comes from a singular run (required multipled kernel restarts) which is to explain why the values in cell is different than analysis (different splits).\n",
    "\n",
    "Key points of analysis:\n",
    "1. KNN failed for the soybean dataset due to its size\n",
    "2. NB failed for the bank dataset given its contents contained negative values.\n",
    "3. SVM for dataset 3 took significant times and had iteration and hang time problems which made it difficult to even evaluate.\n",
    "\n",
    "soybean:\n",
    "1. Logistic, Naive bayes, and random forest achieved perfect scores with logistic and NB being significantly faster than random forest\n",
    "\n",
    "iris:\n",
    "1. KNN, Logistic, and Naive bayes had the best performance with KNN and NB being the fastest\n",
    "\n",
    "skin:\n",
    "1. KNN was by far the best accuracy and fastest with random forests following closely behind.\n",
    "\n",
    "bank:\n",
    "1. Every model had similar accuracy for the tested and even the naive approach had a similar result to the models indicating that perhaps the data structure meant the models could not learn much from the underlying function. Overall based on the equal accuracies naive (not naive bayes) and KNN would be fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e385250e430bb",
   "metadata": {},
   "source": [
    "I think the results above indicate that size (tall versus wide) as well as the type of data contained in the dataset play a large role in what model to select. Additionally, the conditioning of the data in general seems incredibly important or a result like the bank dataset might occur (naive model beating or equalling intensive models like random forest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b8fc96b991540f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
